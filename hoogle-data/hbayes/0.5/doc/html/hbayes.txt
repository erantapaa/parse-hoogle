-- Hoogle documentation, generated by Haddock
-- See Hoogle, http://www.haskell.org/hoogle/


-- | Bayesian Networks
--   
--   Algorithms for Bayesian Networks. It is a very preliminary version. It
--   has only been tested on very simple examples where it worked. It
--   should be considered as experimental and not used in any production
--   work.
@package hbayes
@version 0.5


-- | Bucket algorithms for variable elimination with enough flexibility to
--   also work with influence diagrams.
module Bayes.VariableElimination.Buckets

-- | Used for bucket elimination. Factor are organized by their first DV
data Buckets f
Buckets :: !EliminationOrder DV -> !Map DV [f] -> Buckets f

-- | Elimination order
type EliminationOrder dv = [dv]

-- | Operations needed to process a bucket items
class IsBucketItem f
scalarItem :: IsBucketItem f => f -> Bool
itemProduct :: IsBucketItem f => [f] -> f
itemProjectOut :: IsBucketItem f => DV -> f -> f
itemContainsVariable :: IsBucketItem f => f -> DV -> Bool
createBuckets :: IsBucketItem f => [f] -> EliminationOrder DV -> EliminationOrder DV -> Buckets f

-- | Get the factors for a bucket
getBucket :: DV -> Buckets f -> [f]

-- | Update bucket
updateBucket :: IsBucketItem f => DV -> f -> Buckets f -> Buckets f

-- | Add a factor to the right bucket
addBucket :: IsBucketItem f => Buckets f -> f -> Buckets f

-- | Remove a variable from the bucket
removeFromBucket :: DV -> Buckets f -> Buckets f
marginalizeOneVariable :: IsBucketItem f => Buckets f -> DV -> Buckets f
instance Show f => Show (Buckets f)


-- | Factors
module Bayes.Factor

-- | A factor as used in graphical model It may or not be a probability
--   distribution. So it has no reason to be normalized to 1
class Factor f where factorMainVariable f = let vars = factorVariables f in case vars of { [] -> error "Can't get the main variable of a scalar factor" (h : _) -> h } factorDivide f d = (1.0 / d) `factorScale` f factorProjectTo s f = let alls = factorVariables f s' = alls `difference` s in factorProjectOut s' f
isScalarFactor :: Factor f => f -> Bool
emptyFactor :: Factor f => f
containsVariable :: Factor f => f -> DV -> Bool
factorVariables :: Factor f => f -> [DV]
factorMainVariable :: Factor f => f -> DV
factorWithVariables :: Factor f => [DV] -> [Double] -> Maybe f
factorValue :: Factor f => f -> [DVI] -> Double
factorStringValue :: Factor f => f -> [DVI] -> String
variablePosition :: Factor f => f -> DV -> Maybe Int
factorDimension :: Factor f => f -> Int
factorNorm :: Factor f => f -> Double
factorScale :: Factor f => Double -> f -> f
factorFromScalar :: Factor f => Double -> f
evidenceFrom :: Factor f => [DVI] -> Maybe f
isUsingSameVariablesAs :: Factor f => f -> f -> Bool
factorDivide :: Factor f => f -> Double -> f
factorToList :: Factor f => f -> [Double]
factorProduct :: Factor f => [f] -> f
factorProjectOut :: Factor f => [DV] -> f -> f
factorProjectTo :: Factor f => [DV] -> f -> f

-- | A distribution which can be used to create a factor
class Distribution d
createFactor :: (Distribution d, Factor f) => [DV] -> d -> Maybe f

-- | Class used to display multidimensional tables
class MultiDimTable f
elementStringValue :: MultiDimTable f => f -> [DVI] -> String
tableVariables :: MultiDimTable f => f -> [DV]

-- | Test equality of two factors taking into account the fact that the
--   variables may be in a different order. In case there is a distinction
--   between conditionned variable and conditionning variables (imposed
--   from the exterior) then this comparison may not make sense. It is a
--   comparison of function of several variables which no special
--   interpretation of the meaning of the variables according to their
--   position.
isomorphicFactor :: Factor f => f -> f -> Bool

-- | Norm the factor
normedFactor :: Factor f => f -> f
displayFactorBody :: MultiDimTable f => f -> String

-- | Change factor in a functor (only factor values should have been
--   changed) It assumes that the variables of a factor are enough to
--   identify it. If the functor is containing several factors with same
--   set of variables then it won't give a meaningful result. So it should
--   be used only on functor derived from a Bayesian Network.
changeFactorInFunctor :: (Factor f, Functor m) => f -> m f -> m f

-- | Structure containing factors which can be replaced. It is making sense
--   when the factors are related to the nodes of a Bayesian network.
class FactorContainer m
changeFactor :: (FactorContainer m, IsBucketItem f, Factor f) => f -> m f -> m f

-- | A Set of variables used in a factor. s is the set and a the variable
class Set s where equal sa sb = (sa `subset` sb) && (sb `subset` sa)
emptySet :: Set s => s a
union :: (Set s, Eq a) => s a -> s a -> s a
intersection :: (Set s, Eq a) => s a -> s a -> s a
difference :: (Set s, Eq a) => s a -> s a -> s a
isEmpty :: Set s => s a -> Bool
isElem :: (Set s, Eq a) => a -> s a -> Bool
addElem :: (Set s, Eq a) => a -> s a -> s a
nbElements :: Set s => s a -> Int
subset :: (Set s, Eq a) => s a -> s a -> Bool
equal :: (Set s, Eq a) => s a -> s a -> Bool

-- | A discrete variable has a number of levels which is required to size
--   the factors
class BayesianVariable v => BayesianDiscreteVariable v
dimension :: BayesianDiscreteVariable v => v -> Int
dv :: BayesianDiscreteVariable v => v -> DV

-- | A Bayesian Variable is a variable part of Bayesian network and so
--   which knows its position : the vertex.
class BayesianVariable v
vertex :: BayesianVariable v => v -> Vertex

-- | Vertex type used to identify a vertex in a graph
newtype Vertex
Vertex :: Int -> Vertex
vertexId :: Vertex -> Int

-- | A discrete variable
data DV
DV :: !Vertex -> !Int -> DV

-- | A typed discrete variable
data TDV s

-- | A set of discrete variables The tag is used to check that an index is
--   used with the right set of DV
newtype DVSet s
DVSet :: [DV] -> DVSet s

-- | Discrete Variable instantiation. A variable and its value
data DVI
type DVISet = [DVI]
class InstantiationValue i v | i -> v
instantiationValue :: InstantiationValue i v => i -> v
toDouble :: InstantiationValue i v => i -> Double

-- | Typed instantiation
tdvi :: Enum s => DVI -> (TDV s, s)

-- | Typed discrete variable
tdv :: DV -> TDV s

-- | Create a discrete variable instantiation for a given discrete variable
setDVValue :: DV -> Int -> DVI

-- | Discrete variable from the instantiation
instantiationVariable :: DVI -> DV
variableVertex :: LabeledVertex l => l -> Vertex

-- | Create a variable instantiation using values from an enumeration
(=:) :: Instantiable d v r => d -> v -> r

-- | Generate all instantiations of variables The DVInt can be in any order
--   so the tag s is not used
forAllInstantiations :: DVSet s -> [[DVI]]

-- | Convert a variable instantation to a factor Useful to create evidence
--   factors
factorFromInstantiation :: Factor f => DVI -> f
instance LabeledVertex DV
instance LabeledVertex DVI
instance FactorContainer []
instance Real a => Distribution [a]


-- | Conditional probability table
--   
--   Conditional Probability Tables and Probability tables
module Bayes.Factor.CPT
type CPT = PrivateCPT (Vector) Double

-- | Soft evidence factor can be used to initialize a factor instance
--   Distribution CPT where createFactor dvs f = factorWithVariables dvs
--   (factorToList f)
changeVariableOrder :: DVSet s -> DVSet s' -> [Double] -> [Double]
cptDivide :: CPT -> CPT -> CPT
cptSum :: [CPT] -> CPT
testProductProject_prop :: CPT -> CPT -> Property
testAssocProduct_prop :: CPT -> CPT -> CPT -> Bool

-- | Test product followed by a projection when the factors have no common
--   variables
testScale_prop :: Double -> CPT -> Bool
testProjectCommut_prop :: CPT -> Property
testScalarProduct_prop :: Double -> CPT -> Bool
testProjectionToScalar_prop :: CPT -> Bool
debugCPT :: (Show (t a), Show a) => PrivateCPT t a -> IO ()
instance MultiDimTable CPT
instance IsBucketItem CPT
instance Factor CPT
instance FactorElement Double
instance Show CPT
instance Arbitrary CPT


-- | Implementation of Max-product factors for MAP queries
module Bayes.Factor.MaxCPT
type MAXCPT = PrivateCPT (Vector) (Double, PossibleInstantiations)
mpeInstantiations :: MAXCPT -> [DVISet]
instance MultiDimTable MAXCPT
instance IsBucketItem MAXCPT
instance Show MAXCPT
instance Factor MAXCPT
instance FactorElement (Double, PossibleInstantiations)


-- | Bayesian Network Library.
--   
--   It is a very preliminary version. It has only been tested on very
--   simple examples where it worked. It should be considered as
--   experimental and not used in any production work.
--   
--   <ul>
--   <li>Look at the <a>Bayes.Examples</a> and
--   <a>Bayes.Examples.Tutorial</a> in this package to see how to use the
--   library.</li>
--   <li>In <a>Bayes.Examples.Influence</a> you'll find additional examples
--   about influence diagrams.</li>
--   <li>In <a>Bayes.Examples.Sampling</a> there are some explanations
--   about the samplers for discrete networks.</li>
--   <li><a>Bayes.Examples.EMTest</a> is explaining learning with
--   expectation / maximization.</li>
--   <li><a>Bayes.Examples.ContinuousSampling</a> is showing an example of
--   sampling with a continuous network.</li>
--   </ul>
module Bayes

-- | Graph class used for graph processing algorithms. A graph processing
--   algorithm does not have to know how the graph is implemented nor if it
--   is directed or undirected
class Graph g where endVertex g e = do { (_, ve) <- edgeVertices g e; return ve } startVertex g e = do { (vs, _) <- edgeVertices g e; return vs } isEmpty g = hasNoVertices g && hasNoEdges g
addVertex :: Graph g => Vertex -> b -> g a b -> g a b
removeVertex :: Graph g => Vertex -> g a b -> g a b
vertexValue :: Graph g => g a b -> Vertex -> Maybe b
changeVertexValue :: Graph g => Vertex -> b -> g a b -> Maybe (g a b)
someVertex :: Graph g => g a b -> Maybe Vertex
hasNoVertices :: Graph g => g a b -> Bool
allVertices :: Graph g => g a b -> [Vertex]
allVertexValues :: Graph g => g a b -> [b]
allNodes :: Graph g => g a b -> [(Vertex, b)]
isLinkedWithAnEdge :: Graph g => g a b -> Vertex -> Vertex -> Bool
addEdge :: Graph g => Edge -> a -> g a b -> g a b
removeEdge :: Graph g => Edge -> g a b -> g a b
edgeVertices :: Graph g => g a b -> Edge -> Maybe (Vertex, Vertex)
edgeValue :: Graph g => g a b -> Edge -> Maybe a
someEdge :: Graph g => g a b -> Maybe Edge
hasNoEdges :: Graph g => g a b -> Bool
endVertex :: Graph g => g a b -> Edge -> Maybe Vertex
startVertex :: Graph g => g a b -> Edge -> Maybe Vertex
allEdges :: Graph g => g a b -> [Edge]
allEdgeValues :: Graph g => g a b -> [a]
emptyGraph :: Graph g => g a b
isEmpty :: Graph g => g a b -> Bool
oriented :: Graph g => g a b -> Bool
neighbors :: Graph g => g a b -> Vertex -> Maybe [Vertex]

-- | Undirected graph
class Graph g => UndirectedGraph g
edges :: UndirectedGraph g => g a b -> Vertex -> Maybe [Edge]

-- | Directed graph
class Graph g => DirectedGraph g
ingoing :: DirectedGraph g => g a b -> Vertex -> Maybe [Edge]
outgoing :: DirectedGraph g => g a b -> Vertex -> Maybe [Edge]

-- | The foldable class is limited. For a graph g we may need the vertex in
--   addition to the value
class FoldableWithVertex g
foldrWithVertex :: FoldableWithVertex g => (Vertex -> a -> b -> b) -> b -> g c a -> b
foldlWithVertex' :: FoldableWithVertex g => (b -> Vertex -> a -> b) -> b -> g c a -> b
class FunctorWithVertex g
fmapWithVertex :: FunctorWithVertex g => (Vertex -> a -> b) -> g c a -> g c b
fmapWithVertexM :: (FunctorWithVertex g, Monad m) => (Vertex -> a -> m b) -> g c a -> m (g c b)

-- | A named graph is a graph where the vertices have a name. This name is
--   not a vertex value. Putting this name in the vertex value would make
--   algorithm less readable. A vertex name is only useful to display the
--   graph. Labeled graph has a different meaning in graph theory.
class Graph g => NamedGraph g
addLabeledVertex :: NamedGraph g => String -> Vertex -> b -> g a b -> g a b
vertexLabel :: NamedGraph g => g a b -> Vertex -> Maybe String

-- | Graph monad. The monad used to simplify the description of a new graph
--   g is the graph type. e the edge type. f the node type (generally a
--   <a>Factor</a>)
data GraphMonad g e f a

-- | The state of the graph monad : the graph and auxiliary data useful
--   during the construction
type GMState g e f = (AuxiliaryState, g e f)

-- | Add a node in the graph using the graph monad
graphNode :: NamedGraph g => String -> f -> GraphMonad g e f Vertex
runGraph :: Graph g => GraphMonad g e f a -> (a, g e f)
execGraph :: Graph g => GraphMonad g e f a -> g e f
evalGraph :: Graph g => GraphMonad g e f a -> a
emptyAuxiliaryState :: (Map k a, Int)

-- | Generate a new unique unamed empty variable
getNewEmptyVariable :: NamedGraph g => Maybe String -> f -> GraphMonad g e f Vertex
isRoot :: DirectedGraph g => g a b -> Vertex -> Bool

-- | Get the root node for the graph
rootNode :: DirectedGraph g => g a b -> Maybe Vertex

-- | Return the parents of a node
parentNodes :: DirectedGraph g => g a b -> Vertex -> [Vertex]

-- | Return the children of a node
childrenNodes :: DirectedGraph g => g a b -> Vertex -> [Vertex]

-- | Return the Markov blanket of a node
markovBlanket :: DirectedGraph g => g a b -> Vertex -> [Vertex]

-- | Vertex type used to identify a vertex in a graph
data Vertex

-- | Edge type used to identify and edge in a graph
data Edge

-- | Create an edge description
edge :: Vertex -> Vertex -> Edge

-- | Add a new labeled edge to the graph
newEdge :: Graph g => Vertex -> Vertex -> e -> GraphMonad g e f ()

-- | Get a named vertex from the graph monad
getVertex :: Graph g => String -> GraphMonad g e f (Maybe Vertex)

-- | Endpoints of an edge
edgeEndPoints :: Edge -> (Vertex, Vertex)

-- | Check if the graph is connected
connectedGraph :: Graph g => g a b -> Bool

-- | Check if the graph is a directed Acyclic graph
dag :: DirectedGraph g => g a b -> Bool

-- | Print the values of the graph vertices
printGraphValues :: (Graph (SimpleGraph n), Show b) => SimpleGraph n e b -> IO ()

-- | Directed simple graph
type DirectedSG = SimpleGraph DE

-- | Undirected simple graph
type UndirectedSG = SimpleGraph UE

-- | An implementation of the BayesianNetwork using the simple graph and no
--   value for the edges
type SBN f = DirectedSG () f

-- | Get the variable name mapping
varMap :: SimpleGraph n e v -> Map String Vertex
displaySimpleGraph :: (Vertex -> n -> Maybe String) -> (Vertex -> n -> Maybe String) -> (Edge -> e -> Maybe String) -> (Edge -> e -> Maybe String) -> SimpleGraph local e n -> String

-- | Bayesian network. g must be a directed graph and f a factor
type BayesianNetwork g f = g () f
testEdgeRemoval_prop :: DirectedSG String String -> Property
testVertexRemoval_prop :: DirectedSG String String -> Property
instance Monad (GraphMonad g e f)
instance MonadState (GMState g e f) (GraphMonad g e f)
instance (Show b, Show e) => Show (UndirectedSG e b)
instance Show (DirectedSG String String)
instance Show (DirectedSG () MAXCPT)
instance Show (DirectedSG () CPT)
instance DirectedGraph DirectedSG
instance UndirectedGraph UndirectedSG
instance Graph UndirectedSG
instance Graph DirectedSG
instance NamedGraph UndirectedSG
instance NamedGraph DirectedSG
instance FoldableWithVertex (SimpleGraph local)
instance Traversable (SimpleGraph local edge)
instance Foldable (SimpleGraph local edge)
instance FunctorWithVertex (SimpleGraph local)
instance Functor (SimpleGraph local edge)
instance FactorContainer (SimpleGraph local edge)
instance (Eq a, Eq b) => Eq (SimpleGraph DE a b)
instance NeighborhoodStructure UE
instance NeighborhoodStructure DE
instance Factor f => Arbitrary (DirectedSG () f)
instance Arbitrary (DirectedSG () String)
instance Arbitrary (DirectedSG String String)


-- | Algorithms for variable elimination
module Bayes.VariableElimination

-- | Compute the prior marginal. All the variables in the elimination order
--   are conditionning variables ( p( . | conditionning variables) )
priorMarginal :: (Graph g, IsBucketItem f, Factor f, Show f, BayesianDiscreteVariable dva, BayesianDiscreteVariable dvb) => BayesianNetwork g f -> EliminationOrder dva -> EliminationOrder dvb -> f
posteriorMarginal :: (Graph g, IsBucketItem f, Factor f, Show f, BayesianDiscreteVariable dva, BayesianDiscreteVariable dvb) => BayesianNetwork g f -> EliminationOrder dva -> EliminationOrder dvb -> [DVI] -> f

-- | Compute the interaction graph of the BayesianNetwork
interactionGraph :: (FoldableWithVertex g, Factor f, UndirectedGraph g') => BayesianNetwork g f -> g' () DV

-- | Compute the degree order of an elimination order
degreeOrder :: (FoldableWithVertex g, Factor f, Graph g) => BayesianNetwork g f -> EliminationOrder DV -> Int

-- | Elimination order minimizing the degree
minDegreeOrder :: (Graph g, Factor f, FoldableWithVertex g) => BayesianNetwork g f -> EliminationOrder DV

-- | Elimination order minimizing the filling
minFillOrder :: (Graph g, Factor f, FoldableWithVertex g) => BayesianNetwork g f -> EliminationOrder DV

-- | Get all variables from a Bayesian Network
allVariables :: (Graph g, Factor f) => BayesianNetwork g f -> [DV]

-- | Compute the prior marginal. All the variables in the elimination order
--   are conditionning variables ( p( . | conditionning variables) )
marginal :: (IsBucketItem f, Factor f) => [f] -> EliminationOrder DV -> EliminationOrder DV -> [DVI] -> f

-- | Compute the prior marginal. All the variables in the elimination order
--   are conditionning variables ( p( . | conditionning variables) ) First
--   we sum, then we maximize for the remaining variables
mpemarginal :: [CPT] -> EliminationOrder DV -> EliminationOrder DV -> [DVI] -> MAXCPT

-- | Most Probable Explanation (or Maximum A Posteriori estimator) when
--   restricted to a subest of variables in output
mpe :: (Graph g, BayesianDiscreteVariable dva, BayesianDiscreteVariable dvb) => BayesianNetwork g CPT -> EliminationOrder dva -> EliminationOrder dvb -> [DVI] -> [DVISet]

-- | Elimination order
type EliminationOrder dv = [dv]


-- | Import / export Bayesian networks and junction tress
module Bayes.ImportExport

-- | Write a bayesian network to file
writeNetworkToFile :: FilePath -> SBN CPT -> IO ()

-- | Read bayesian network from file
readNetworkFromFile :: FilePath -> IO (SBN CPT)

-- | Write a junction tree and the variable map to a file
writeVariableMapAndJunctionTreeToFile :: FilePath -> (Map String Vertex) -> JunctionTree CPT -> IO ()

-- | Read variable map and junction tree from file
readVariableMapAndJunctionTreeToFile :: FilePath -> IO (Map String Vertex, JunctionTree CPT)
instance Binary UE
instance Binary DE
instance (Binary l, Binary e, Binary v) => Binary (SimpleGraph l e v)
instance Binary Edge
instance Binary Vertex
instance Binary (v Double) => Binary (PrivateCPT v Double)
instance Binary DV
instance Binary (Vector Double)
instance Binary (Vector Double)
instance Binary a => Binary (SeparatorValue a)
instance Binary a => Binary (NodeValue a)
instance (Ord c, Binary c, Binary f) => Binary (JTree c f)
instance Binary Cluster


-- | Algorithms for factor elimination
module Bayes.FactorElimination

-- | For the junction tree construction, only the vertices are needed
--   during the intermediate steps. So, the moral graph is returned without
--   any vertex data.
moralGraph :: (NamedGraph g, FoldableWithVertex g, DirectedGraph g) => g () b -> UndirectedSG () b

-- | Node selection comparison function used for triangulating the graph
nodeComparisonForTriangulation :: (UndirectedGraph g, Factor f) => g a f -> Vertex -> Vertex -> Ordering

-- | Number of edges added when connecting all neighbors
numberOfAddedEdges :: UndirectedGraph g => g a b -> Vertex -> Integer

-- | Weight of a node
weight :: (UndirectedGraph g, Factor f) => g a f -> Vertex -> Integer
weightedEdges :: (UndirectedGraph g, Factor f) => g a f -> Vertex -> Integer

-- | Triangulate a graph using a cost function The result is the
--   triangulated graph and the list of clusters which may not be maximal.
triangulate :: Graph g => (Vertex -> Vertex -> Ordering) -> g () b -> [VertexCluster]

-- | Create the cluster graph
createClusterGraph :: (UndirectedGraph g, Factor f, Graph g') => g' e f -> [VertexCluster] -> g Int Cluster

-- | Cluster of discrete variables. Discrete variables instead of vertices
--   are needed because the factor are using <a>DV</a> and we need to find
--   which factors must be contained in a given cluster.
data Cluster

-- | Create a function tree
createJunctionTree :: (DirectedGraph g, FoldableWithVertex g, NamedGraph g, Factor f, IsBucketItem f, Show f) => (UndirectedSG () f -> Vertex -> Vertex -> Ordering) -> BayesianNetwork g f -> JunctionTree f

-- | Create a junction tree with only the clusters and no factors
createUninitializedJunctionTree :: (DirectedGraph g, FoldableWithVertex g, NamedGraph g, Factor f, Show f) => (UndirectedSG () f -> Vertex -> Vertex -> Ordering) -> g () f -> JunctionTree f
type JunctionTree f = JTree Cluster f

-- | Display the tree values
displayTreeValues :: (Show f, Show c) => JTree c f -> IO ()

-- | Collect message taking into account that the tree depth may be
--   different for different leaves.
collect :: (Ord c, Message a c) => JTree c a -> JTree c a
distribute :: (Ord c, Message a c) => JTree c a -> JTree c a

-- | Compute the marginal posterior (if some evidence is set on the
--   junction tree) otherwise compute just the marginal prior. The set of
--   variables must be included inside a cluster for thr algorithm to work.
--   So, most of the cases, it will be used to compute the posterior of
--   just one variable.
posterior :: (BayesianDiscreteVariable dv, Factor f, IsBucketItem f) => JunctionTree f -> [dv] -> Maybe f

-- | Change evidence in the network
changeEvidence :: (IsCluster c, Ord c, Factor f, Message f c, Show c, Show f) => [DVI] -> JTree c f -> JTree c f
junctionTreeProperty_prop :: DirectedSG () CPT -> Property
junctionTreeAllClusters_prop :: DirectedSG () CPT -> Property

-- | A cluster containing only the vertices and not yet the factors
data VertexCluster
junctionTreeProperty :: JTree Cluster CPT -> [Cluster] -> Cluster -> Bool

-- | Implementing the Prim's algorithm for minimum spanning tree
maximumSpanningTree :: (UndirectedGraph g, IsCluster c, Factor f, Ord c, Show c, Show f) => g Int c -> JTree c f
fromVertexCluster :: VertexCluster -> Set Vertex
triangulatedebug :: Graph g => (Vertex -> Vertex -> Ordering) -> g () b -> ([VertexCluster], [g () b])
instance Eq VertexCluster
instance Ord VertexCluster
instance Show VertexCluster


-- | Module for building Bayesian Networks
module Bayes.BayesianNetwork

-- | The Bayesian monad
type BNMonad g f a = NetworkMonad g () f a

-- | Create a network using the simple graph implementation The initialized
--   nodes are replaced by the value. Returns the monad values and the
--   built graph.
runBN :: BNMonad DirectedSG f a -> (a, SBN f)

-- | Create a bayesian network but only returns the monad value. Mainly
--   used for testing.
evalBN :: BNMonad DirectedSG f a -> a

-- | Create a network but only returns the monad value. Mainly used for
--   testing.
execBN :: BNMonad DirectedSG f a -> SBN f

-- | A distribution which can be used to create a factor
class Distribution d
createFactor :: (Distribution d, Factor f) => [DV] -> d -> Maybe f

-- | Define a Bayesian variable (name and bounds)
variable :: (Enum a, Bounded a, NamedGraph g) => String -> a -> NetworkMonad g e f (TDV a)

-- | Create a new unamed variable
unamedVariable :: (Enum a, Bounded a, NamedGraph g) => a -> NetworkMonad g e f (TDV a)

-- | Define a Bayesian variable (name and bounds)
variableWithSize :: NamedGraph g => String -> Int -> NetworkMonad g e f DV

-- | Typed discrete variable
tdv :: DV -> TDV s

-- | Synonym for undefined because it is clearer to use t to set the Enum
--   bounds of a variable
t :: a

-- | Define a conditional probability between different variables Variables
--   are ordered like FFF FFT FTF FTT TFF TFT TTF TTT and same for other
--   enumeration keeping enumeration order Note that the reverse is
--   important. We add the parents in such a way that <a>ingoing</a> will
--   give a list of parents in the right order. This order must correspond
--   to the order of values in the initialization.
cpt :: (DirectedGraph g, BayesianDiscreteVariable v, BayesianDiscreteVariable vb) => v -> [vb] -> BNMonad g f v

-- | Define proba for a variable Values are ordered like FFF FFT FTF FTT
--   TFF TFT TTF TTT and same for other enumeration keeping enumeration
--   order
proba :: (DirectedGraph g, BayesianDiscreteVariable v) => v -> BNMonad g f v

-- | Initialize the values of a factor
(~~) :: (DirectedGraph g, Factor f, Distribution d, BayesianDiscreteVariable v) => BNMonad g f v -> d -> BNMonad g f ()

-- | Create an auxiliairy node to force soft evidence
softEvidence :: (NamedGraph g, DirectedGraph g, Factor f) => TDV Bool -> BNMonad g f (TDV Bool)

-- | Soft evidence factor
se :: Factor f => TDV s -> TDV s -> Double -> Maybe f
logical :: (Factor f, DirectedGraph g) => TDV Bool -> LE -> BNMonad g f ()

-- | Create a variable instantiation using values from an enumeration
(.==.) :: Testable d v => d -> v -> LE
(.!.) :: LE -> LE
(.|.) :: LE -> LE -> LE
(.&.) :: LE -> LE -> LE

-- | Noisy OR. The Noisy-OR with leak can be implemented by using the
--   standard Noisy-OR and a leak variable.
noisyOR :: (DirectedGraph g, Factor f, NamedGraph g) => [(TDV Bool, Double)] -> BNMonad g f (TDV Bool)
instance Eq LE
instance Instantiable d v DVI => Testable d v


-- | Parser for a subset of the Hugin Net language
module Bayes.ImportExport.HuginNet

-- | Import a bayesian network form a Hugin file. Only a subset of the file
--   format is supported. You may have to convert the line endings to be
--   able to parse a file When it is succeeding, it is returing a bayesian
--   network monad and a mapping from node names to discrete variables.
importBayesianGraph :: Factor f => String -> IO (Maybe (BNMonad DirectedSG f (Map String DV)))
instance Eq Section
instance Show Section


-- | Examples of networks
--   
--   <i>Creating a simple network</i>
--   
--   The <a>example</a> function is the typical example. It is using the
--   monad <a>BNMonad</a>. The goal of this monad is to offer a way of
--   describing the network which is natural.
--   
--   There are only three functions to understand inside the monad:
--   
--   <ul>
--   <li><a>variable</a> to create a discrete variable of type <a>DV</a>.
--   Creating a discrete variable is using a <a>Bounded</a> and <a>Enum</a>
--   type like for instance <a>Bool</a>.</li>
--   <li><a>proba</a> to define the probability P(A) of a variable A</li>
--   <li><a>cpt</a> to define the conditional probability table P(A |
--   BC)</li>
--   </ul>
--   
--   It is important to understand how the values are organized. If you
--   define P( wet | sprinkler road) then you have to give the values in
--   the order:
--   
--   <pre>
--   wet=False, sprinkler=False, road=False
--   wet=False, sprinkler=False, road=True
--   wet=False, sprinkler=True, road=False
--   wet=False, sprinkler=True, road=True
--   </pre>
--   
--   Finally, don't forget to return the discrete variables at the end of
--   your network construction because those variables are used for making
--   inferences.
--   
--   <pre>
--   example :: ([<a>TDV</a> Bool],<a>SBN</a> <a>CPT</a>)
--   example = <a>runBN</a> $ do 
--       winter &lt;- <a>variable</a> "winter" (t :: Bool)
--       sprinkler &lt;- <a>variable</a> "sprinkler" (t :: Bool) 
--       wet &lt;- <a>variable</a> "wet grass" (t :: Bool) 
--       rain &lt;- <a>variable</a> "rain" (t :: Bool) 
--       road &lt;- <a>variable</a> "slippery road" (t :: Bool) 
--   --
--       <a>proba</a> winter ~~ [0.4,0.6]
--       <a>cpt</a> sprinkler [winter] ~~ [0.25,0.8,0.75,0.2]
--       <a>cpt</a> rain [winter] ~~ [0.9,0.2,0.1,0.8]
--       <a>cpt</a> wet [sprinkler,rain] ~~ [1,0.2,0.1,0.05,0,0.8,0.9,0.95]
--       <a>cpt</a> road [rain] ~~ [1,0.3,0,0.7]
--       return [winter,sprinkler,rain,wet,road]
--   </pre>
--   
--   By default, all variables are typed (<a>TDV</a> Bool). <a>TDV</a>
--   means Typed Discrete Variable.
--   
--   In case you are mixing several types, you'll need to remove the type
--   to build the <a>cpt</a> since the list can't be heterogeneous. Just
--   use <a>dv</a> for this. It will convert the variable into the type
--   <a>DV</a> of untyped discrete variable.
--   
--   <i>Creating truth tables</i>
--   
--   In practise, it is easy to compute the posterior of a variable because
--   it is always possible to find a cluster containing the variable in the
--   junction tree. But, it is more difficult to compute the posterior of a
--   logical assertion or just a conjunction of assertions.
--   
--   If a query is likely to be done often, then it may be a good idea to
--   add a new node to the Bayesian network to represent this query. So,
--   some functions to create truth tables are provided.
--   
--   <pre>
--   exampleLogical :: ([<a>TDV</a> Bool], <a>SBN</a> <a>CPT</a>)
--   exampleLogical = <a>runBN</a> $ do 
--       a &lt;- <a>variable</a> "a" (t :: Bool)
--       b &lt;- <a>variable</a> "b" (t :: Bool)
--       notV &lt;- <a>variable</a> "notV" (t :: Bool)
--       andV &lt;- <a>variable</a> "andV" (t :: Bool)
--       orV &lt;- <a>variable</a> "orV" (t :: Bool)
--       let ta = a <a>.==.</a> True 
--           tb = b <a>.==.</a> True
--       <a>logical</a> notV ((<a>.!.</a>) ta)
--       <a>logical</a> andV (ta <a>.&amp;.</a> tb)
--       <a>logical</a> orV (ta <a>.|.</a> tb)
--       return $ [a,b,notV,andV,orV]
--   </pre>
--   
--   In the previous example, we force a type on the discrete variables
--   <a>DV</a> to avoid futur errors in the instantiations. It is done
--   through the <a>tdv</a> function.
--   
--   But, it is also possible to use the untyped variables and write:
--   
--   <pre>
--   <a>logical</a> andV ((a <a>.==.</a> True) <a>.&amp;.</a> (b <a>.==.</a> True))
--   </pre>
--   
--   The goal of a Bayesian network is to factorize a big probability table
--   because otherwise the algorithms can't process it. So, of course it is
--   not a good idea to represent a complex logical assertion with a huge
--   probability table. So, the <a>logical</a> keyword should only be used
--   to build small tables.
--   
--   If you need to encode a complex logical assertion, use <a>logical</a>
--   several times to build a network representing the assertion instead of
--   building just one node to represent it.
--   
--   <i>Noisy OR</i>
--   
--   The Noisy OR is a combination of logical tables (OR) and conditional
--   probability tables which is often used during modeling to avoid
--   generating big conditional probability tables.
--   
--   It is easy to use:
--   
--   <pre>
--   no &lt;- <a>noisyOR</a> [(a,0.1),(b,0.2),(c,0.3)] 
--   </pre>
--   
--   Each probability is the probability that a given variable has no
--   effect (so is inhibited in the OR).
--   
--   <i>Importing a network from a Hugin file</i>
--   
--   The <a>exampleImport</a> function can be used to import a file in
--   Hugin format. Only a subset of the format is supported. The function
--   will return a mapping from node names to Discrete Variables <a>DV</a>.
--   The node name is used and not the node's label. The function is also
--   returning a simple bayesian network <a>SBN</a> using <a>CPT</a> as
--   factors.
--   
--   The implementation is using <a>getDataFileName</a> to find the path of
--   the test pattern installed by cabal.
--   
--   <pre>
--   exampleImport :: IO (Map.Map String <a>DV</a>,<a>SBN</a> <a>CPT</a>)
--   exampleImport = do 
--       path &lt;- <a>getDataFileName</a> "cancer.net"
--       r &lt;- <a>importBayesianGraph</a> path
--       return (<a>runBN</a> $ fromJust r)
--   </pre>
module Bayes.Examples

-- | Standard example found in many books about Bayesian Networks.
example :: ([TDV Bool], SBN CPT)
exampleJunction :: UndirectedSG () Vertex

-- | Standard example but with a wrong factor that is changed in the tests
--   using factor replacement functions
exampleWithFactorChange :: ([TDV Bool], SBN CPT)

-- | Example of soft evidence use
exampleSoftEvidence :: ((TDV Bool, TDV Bool), SBN CPT)

-- | Example showing how to import a graph described into a Hugin file.
exampleImport :: IO (Map String DV, SBN CPT)

-- | Diabete example (not provided with this package)
exampleDiabete :: IO (Map String DV, SBN CPT)

-- | Asia example (not provided with this package)
exampleAsia :: IO (Map String DV, SBN CPT)

-- | Poker example (not provided with this package)
examplePoker :: IO (Map String DV, SBN CPT)

-- | Farm example (not provided with this package)
exampleFarm :: IO (Map String DV, SBN CPT)

-- | Perso example (not provided with this package)
examplePerso :: IO (Map String DV, SBN CPT)
exampleLogical :: ([TDV Bool], SBN CPT)
testJunction :: DirectedSG () Vertex
anyExample :: FilePath -> IO (Map String DV, SBN CPT)


-- | A comparison of variable elimination and factor elimination on a
--   simple graph.
--   
--   It is a non regression test.
module Bayes.Test.CompareEliminations

-- | Compare that variable elemination and factor elimination are giving
--   similar results on a simple example
compareVariableFactor :: IO ()
compareFactorChange :: IO ()


-- | A comparison of factor elimination with reference values generated
--   with another bayesian network software
--   
--   It is a non regression test. The test patterns are not provided with
--   this package. So, those tests are disabled by default in the hackage
--   version.
module Bayes.Test.ReferencePatterns

-- | Test that we can import / export the bayesian network, junction tree
--   and variable map
testFileExport :: IO ()


-- | Tutorial explaining how to make infereces with the library.
--   
--   Thus tutorial is using examples from the module <a>Bayes.Examples</a>.
--   Please, refer to this module for documentation about how the example
--   bayesian networks are created or loaded.
--   
--   <i>Inferences</i>
--   
--   The function <a>inferencesOnStandardNetwork</a> is showing how to use
--   variable elimination and factor elimination to make inferences.
--   
--   First, the <a>example</a> is loaded to make its variables and its
--   bayesian network available:
--   
--   <pre>
--   let ([winter,sprinkler,rain,wet,road],exampleG) = example
--   </pre>
--   
--   Then, we compute a prior marginal. Prior means that no evidence is
--   used. A bayesian network is a factorisation of a distribution P(A B C
--   ...). If you want to know the probability of only A, you need to sum
--   out the other variables to eliminate them and get P(A). To compute
--   this prior marginal using variable elimnation, you need to give an
--   elimination order. The complexity of the computation is depending on
--   the elimination order chosen.
--   
--   For instance, if you want to compute the prior probability of rain,
--   you can write:
--   
--   <pre>
--   <a>priorMarginal</a> exampleG [winter,sprinkler,wet,road] [rain] 
--   </pre>
--   
--   Now, if you have observed that the grass is wet and want to take into
--   account thios observation to compute the posterior probability of rain
--   (after observation):
--   
--   <pre>
--   <a>posteriorMarginal</a> exampleG [winter,sprinkler,wet,road] [rain]  [wet <a>=:</a> True]
--   </pre>
--   
--   If you want to combine several observations:
--   
--   <pre>
--   <a>posteriorMarginal</a> exampleG [winter,sprinkler,wet,road] [rain]  [wet <a>=:</a> True, sprinkler <a>=:</a> True]
--   </pre>
--   
--   There are several problems with variable elimination:
--   
--   <ul>
--   <li>You have to specify an elimination order</li>
--   <li>If you want to compute another marginal (for instance probability
--   of winter), you have to recompute everything.</li>
--   </ul>
--   
--   But, there exists another category of elimination algorithms based
--   upon factor elimination. They require the creation of an auxiliary
--   data structure : the junction tree.
--   
--   This tree is then used for computing all marginals (without having to
--   recompute everything). The junction tree is equivalent to giving an
--   elimination order.
--   
--   So, the previous examples can also be computed with factor
--   elimination. First, the junction tree must created:
--   
--   <pre>
--   let jt = <a>createJunctionTree</a> <a>nodeComparisonForTriangulation</a> exampleG
--   </pre>
--   
--   The junction tree being equivalent to an elimination order, the order
--   chosen will depend on a cost function. In the previous example, the
--   cost function <a>nodeComparisonForTriangulation</a> is used. Other
--   cost functions may be introduced in a futute version of this library.
--   
--   Once the junction tree has been computd, it can be used to compute
--   several marginals:
--   
--   <pre>
--   <a>posterior</a> jt [rain]
--   </pre>
--   
--   The function is called posterior and will compute posterior only when
--   solme evidence has been introduced into the tree. Otherwise it is
--   computing a prior.
--   
--   To set evidence, you need to update the junction tree with new
--   evidence:
--   
--   <pre>
--   let jt' = <tt>updateEvidence</tt> [wet <a>=:</a> True] jt 
--   <a>posterior</a> jt' [rain]
--   </pre>
--   
--   If you want to compute the posterior for a combination of variables,
--   you have two possibilities : either going back to the variable
--   elimination methods. Or, introduce new nodes in the network to
--   represent the query.
--   
--   It is easily done through the new <tt>logical</tt> function when
--   building the Bayesian graph.
--   
--   Once you have a node to represent a complex query, you can use it to
--   compute a posterior. For instance, in the rain example, there is a new
--   variable:
--   
--   <pre>
--   roadandrain &lt;- <tt>variable</tt> "rain and slippery road" (t :: Bool)
--   <tt>logical</tt> roadandrain ((rain <tt>.==.</tt> True) <tt>.&amp;.</tt> (road <tt>.==.</tt> True))
--   </pre>
--   
--   This variable is representing the assertion : rain True AND slippery
--   road True. This variable can be used to answer different queries, like
--   for instance:
--   
--   <pre>
--       let jt4 = <a>changeEvidence</a> [wet <a>=:</a> True] jt 
--       print "Posterior Marginal : probability of rain and road slippery if grass wet"
--       let m = <a>posterior</a> jt4 [roadandrain]
--       print m
--   --
--       let jt5 = <a>changeEvidence</a> [wet <a>=:</a> True, sprinkler <a>=:</a> False] jt 
--       print "Posterior Marginal : probability of rain and road slippery if grass wet and srinkler not used"
--       let m = <a>posterior</a> jt5 [roadandrain]
--   </pre>
--   
--   <i>Inferences with an imported network</i>
--   
--   There is a slight additional difficulty with imported networks : you
--   need to create new data type to be able to set evidence.
--   
--   For instance, in the cancer network there is a Coma variable with
--   levels Present or Absent. When imported, those levels are imported as
--   number. But, the evidence API in this library is requiring
--   enumerations.
--   
--   So, you need to create a <a>Coma</a> type:
--   
--   <pre>
--   data Coma = Present | Absent deriving(Eq,Enum,Bounded)
--   </pre>
--   
--   and check that <a>Present</a> is corresponding to the level 0 in the
--   imported network.
--   
--   Once this datatype is created, you can easily use the cancer network.
--   First we load the network and import the discrete variables of type
--   <a>DV</a> from the names of the nodes in the network (not the label of
--   the nodes).
--   
--   <pre>
--   print "CANCER NETWORK"
--   (varmap,cancer) &lt;- <a>exampleImport</a>
--   print cancer
--   let [varA,varB,varC,varE] = fromJust $ mapM (flip Map.lookup varmap) ["A","B","C","E"]
--   </pre>
--   
--   To avoid any errors with the future queries, some imported variables
--   can be transformed into typed variables:
--   
--   <pre>
--   varD = <a>tdv</a> (fromJust $ Map.lookup "D" varmap) :: <a>TDV</a> Coma
--   </pre>
--   
--   Once the variables are available, you can create the junction tree and
--   start making inferences:
--   
--   <pre>
--       let jtcancer = <a>createJunctionTree</a> <a>nodeComparisonForTriangulation</a> cancer
--   --
--       mapM_ (x -&gt; putStrLn (show x) &gt;&gt; (print . <a>posterior</a> jtcancer $ [x])) [varA,varB,varC,varE]
--   --
--       print "UPDATED EVIDENCE"
--       let jtcancer' = <tt>updateEvidence</tt> [varD <a>=:</a> Present] jtcancer 
--       mapM_ (x -&gt; putStrLn (show x) &gt;&gt; (print . <a>posterior</a> jtcancer' $ [x])) [varA,varB,varC,varE]
--   </pre>
--   
--   The <a>=:</a> operator will check that the assignment is type
--   compatible because varD is a typed discrete variable of type
--   <a>TDV</a> Coma.
--   
--   <i>MPE inferences</i>
--   
--   It is possible to compute the Most Probable Explanation for a set of
--   observation. The syntax is very similar to the posterior computation
--   with variable elimination:
--   
--   <pre>
--   let m = <a>mpe</a> exampleG [wet,road] [winter,sprinkler,rain,roadandrain] [wet <a>=:</a> True, road <a>=:</a> True]
--   </pre>
--   
--   The first list of variables (which should containg the evidence
--   variables) is summed out. The second list of variables is used to
--   maximize the probability. Both lists should contain all variables of
--   the Bayesian network and are defining an elimination order.
--   
--   The result of the mpe functions is a list of instantiations. The
--   result is easier to read when the type information is reintroduced. It
--   can be done with the <a>tdvi</a> function:
--   
--   <pre>
--   let typedResult = map (map <a>tdvi</a>) m :: [[(<a>TDV</a> Bool,Bool)]]
--   </pre>
--   
--   In this example, all variables are boolean ones.
--   
--   <i> Soft Evidence </i>
--   
--   Soft evidence is more complex to handle since new node have to be
--   added to the graph. And the node factor has to be changed when the
--   node evidence is changed.
--   
--   Here is how you could do it. First you load an example graph containg
--   a soft evidence node created with <tt>softEvidence</tt>.
--   
--   <pre>
--   inferencesWithSoftEvidence = do 
--       let ((a,seNode),exampleG) = <a>exampleSoftEvidence</a> 
--   </pre>
--   
--   Then, you create the junction tree as usual and force an hard evidence
--   on the soft evidence node.
--   
--   <pre>
--   jt = <a>createJunctionTree</a> <a>nodeComparisonForTriangulation</a> exampleG
--   jt' = <a>changeEvidence</a> [seNode <a>=:</a> True] jt
--   </pre>
--   
--   This junction tree cannot be used because the soft evidence node
--   created in <a>exampleSoftEvidence</a> has a probability table which is
--   meaningless. You need to update the probability table for a given soft
--   evidence. You create a new factor for this:
--   
--   <pre>
--   theNewFactor x = fromJust $ <a>se</a> seNode a x -- x % success for the sensor
--   </pre>
--   
--   This new factor, can then be used to do inference with different soft
--   evidences.
--   
--   <pre>
--       print "Sensor 90%"
--       print $ posterior (<a>changeFactor</a> (theNewFactor 0.9) jt') [a]
--   --
--       print "Sensor 50%"
--       print $ posterior (<a>changeFactor</a> (theNewFactor 0.5) jt') [a]
--   --
--       print "Sensor 10%"
--       print $ posterior (<a>changeFactor</a> (theNewFactor 0.1) jt') [a]
--   </pre>
module Bayes.Examples.Tutorial

-- | Inferences with the standard network
inferencesOnStandardNetwork :: IO ()

-- | Most likely explanation on standard network
mpeStandardNetwork :: IO ()

-- | Inferences with soft evidence
inferencesWithSoftEvidence :: IO ()

-- | Inferences with the cancer network
inferencesOnCancerNetwork :: IO ()

-- | Type defined to set the evidence on the Coma variable from the cancer
--   network.
data Coma
Present :: Coma
Absent :: Coma
miscTest :: FilePath -> IO ()

-- | Display of factors generated by the logical keyword
logicalTest :: IO ()
instance Eq Coma
instance Enum Coma
instance Bounded Coma


-- | Tools to build influence diagrams
module Bayes.InfluenceDiagram

-- | Influence diagram
type InfluenceDiagram = DirectedSG EdgeKind IDValue
type DecisionFactor = PrivateCPT (Vector) DVI
class Instantiable d v r | d -> r
(=:) :: Instantiable d v r => d -> v -> r

-- | Decision variable
data DEV

-- | Utility variable
data UV

-- | A discrete variable
data DV

-- | A typed discrete variable
data TDV s
type IDMonad g a = NetworkMonad g EdgeKind IDValue a

-- | Synonym for undefined because it is clearer to use t to set the Enum
--   bounds of a variable
t :: a
(~~) :: (Initializable v, DirectedGraph g, Distribution d) => IDMonad g v -> d -> IDMonad g ()

-- | Create a chance node
chance :: (Bounded a, Enum a, NamedGraph g) => String -> a -> IDMonad g (TDV a)

-- | Create a decision node
decisionNode :: (Bounded a, Enum a, NamedGraph g) => String -> a -> IDMonad g DEV

-- | Create an utility node
utilityNode :: NamedGraph g => String -> IDMonad g UV

-- | Define that a chance node is a probability (not conditional) Values
--   are ordered like FFF FFT FTF FTT TFF TFT TTF TTT and same for other
--   enumeration keeping enumeration order
proba :: (ChanceVariable c, DirectedGraph g) => c -> IDMonad g c

-- | Define a decision dependence
decision :: (DirectedGraph g, BayesianDiscreteVariable dv) => DEV -> [dv] -> IDMonad g DEV

-- | Define a utility dependence
utility :: (DirectedGraph g, BayesianDiscreteVariable dv) => UV -> [dv] -> IDMonad g UV

-- | Define that a chance node is a conditional probability and define the
--   parent variables
cpt :: (DirectedGraph g, BayesianDiscreteVariable vb, ChanceVariable c) => c -> [vb] -> IDMonad g c

-- | Used to mix decision and chance variables and a same list
d :: DEV -> PorD

-- | Used to mix decision and chance variables and a same list
p :: ChanceVariable c => c -> PorD

-- | Used to define a root decision which is not dependent on any past node
noDependencies :: [DV]

-- | List of decision vertices in reverse temporal order (corresponding to
--   elimination order)
decisionsOrder :: InfluenceDiagram -> [ChancesOrDecision]

-- | Solve an influence diagram. A DecisionFactor is generated for each
--   decision variable. A decision factor is containing a variable
--   instantiation instead of a double. This instantiation is giving the
--   decision to take for each value of the parents.
solveInfluenceDiagram :: InfluenceDiagram -> [DecisionFactor]

-- | Run an influence monad
runID :: IDMonad DirectedSG a -> (a, InfluenceDiagram)

-- | Create a policy network from an influence diagram and its solution. A
--   policy network is a Bayesian network where the decision nodes have
--   been replaced with probability nodes where the probability is 1 when
--   the configuration is corresponding to the decision and 0 otherwise.
policyNetwork :: [DecisionFactor] -> InfluenceDiagram -> SBN CPT

-- | Convert a decision policy to a set of possible instantiations It is
--   the only way to access to the content of a decision factor.
decisionToInstantiation :: DecisionFactor -> [DVISet]
type DVISet = [DVI]

-- | Discrete Variable instantiation. A variable and its value
data DVI
instance Eq JoinSum
instance Eq EdgeKind
instance Show EdgeKind
instance Eq IDValue
instance Eq UV
instance Eq DEV
instance Ord DEV
instance Eq PorD
instance Eq ChancesOrDecision
instance Ord ChancesOrDecision
instance Show ChancesOrDecision
instance Initializable DEV
instance Initializable UV
instance Initializable (TDV s)
instance Initializable DV
instance BayesianDiscreteVariable PorD
instance BayesianVariable PorD
instance ChanceVariable (TDV s)
instance ChanceVariable DV
instance Instantiable DEV Int DVI
instance BayesianDiscreteVariable DEV
instance BayesianVariable DEV
instance Show DEV
instance Show IDValue
instance Monoid EdgeKind
instance Show InfluenceDiagram
instance IsBucketItem JoinSum
instance Show JoinSum
instance MultiDimTable DecisionFactor
instance Show DecisionFactor


-- | Examples of influence diagrams
--   
--   An influence diagram is an extension of a Bayesian network with can be
--   used to solve some decision problems. In an influence diagram, there
--   are two new kind of nodes : decision nodes and utility nodes.
--   
--   Solving an influence diagram means determining the strategies for each
--   decision variable that will maximize the average utility.
--   
--   There must be an ordering of the decision variables : a path through
--   all the decisions.
--   
--   A decision variable can depend on other past decisions and
--   probabilistic nodes. In the later case, the variable of the
--   probabilistic node is assumed to be observed before the decision is
--   taken. So, the decision is only trying to maximize the average utility
--   based on what has not been observed (the future and some past
--   probabilistic variables).
--   
--   A probabilistic node can depend on other probabilistic nodes (like in
--   a Bayesian network) and decision nodes.
--   
--   An utility is a leaf of the graph.
--   
--   <i>Example graph</i>
--   
--   Building an influence diagram is done like for a Bayesian network : by
--   using the right monad.
--   
--   <pre>
--   import Bayes.InfluenceDiagram 
--   studentSimple = snd . <a>runID</a> $ do
--   </pre>
--   
--   Then, you create the different nodes of the graph:
--   
--   <pre>
--   e &lt;- <a>decisionNode</a> "E" (<a>t</a> :: E)
--   uc &lt;- <a>utilityNode</a> "UC"
--   ub &lt;- <a>utilityNode</a> "UB"
--   i &lt;- <a>chance</a> <a>I</a> (<a>t</a> :: I)
--   pr &lt;- <a>chance</a> <a>P</a> (<a>t</a> :: Bool)
--   </pre>
--   
--   The types used above are:
--   
--   <pre>
--   data E = Dont | Do deriving(Eq,Enum,Bounded)
--   data I = Low | Average | High deriving(Eq,Enum,Bounded)
--   </pre>
--   
--   Then, you need to define the dependencies and the numerical values.
--   For probabilistic nodes, it is done like for Bayesian network:
--   
--   <pre>
--   cpt pr [<a>d</a> e] ~~ [1-0.0000001,1 - 0.001,0.0000001, 0.001]
--   cpt i [<a>p</a> pr, <a>d</a> e] ~~ [0.2,0.1,0.01,0.01,0.6,0.5,0.04,0.04,0.2,0.4,0.95,0.95]
--   </pre>
--   
--   The list may contain decision variables of type <a>DEV</a> and
--   probabilistic variables of type <a>DV</a> or <a>TDV</a>. So, the
--   functions <a>p</a> an <a>d</a> are used for the embedding in the
--   heterogenous list.
--   
--   For decision nodes, the method is similar but with two differences :
--   The first decision may depend on nothing (just on the assumed future).
--   And there are no values to define for a decision variable since the
--   goal of the influence diagram is to compute them.
--   
--   <pre>
--   <a>decision</a> e <a>noDependencies</a>
--   </pre>
--   
--   For the utility nodes, it is similar to probabilistic nodes. You
--   define the dependencies and the numerical values:
--   
--   <pre>
--   <a>utility</a> uc [e] ~~ [0,-50000]
--   <a>utility</a> ub [i] ~~ [100000,200000,500000]
--   </pre>
--   
--   Once the influence diagram is defined, you can solve it:
--   
--   <pre>
--   <a>solveInfluenceDiagram</a> studentSimple
--   </pre>
--   
--   The result of this function is the solution : the decision strategies.
--   You may want to display also the original graph to see to which node
--   are corresponding the vertex numbers.
--   
--   <i>Policy Network</i>
--   
--   You can transform a solved influence diagram into a policy network : a
--   Bayesian network where decision variables have been replaced with
--   probabilistic variables where the conditional probability table is
--   containing 1 for a choice of variables corresponding to the decision
--   and 0 otherwise.
--   
--   <pre>
--   let l = <a>solveInfluenceDiagram</a> student
--       g = <a>policyNetwork</a> l student
--   print g 
--   <a>printGraphValues</a> g
--   </pre>
module Bayes.Examples.Influence

-- | Very simple example with one decision node
exampleID :: InfluenceDiagram

-- | Student network as found in the book by Barber
student :: InfluenceDiagram

-- | Student network as found in the book by Barber
studentSimple :: InfluenceDiagram

-- | Market diagram
market :: InfluenceDiagram
studentDecisionVars :: (DEV, TDV Bool, DEV)
studentSimpleDecisionVar :: DEV

-- | Solve the influences diagrams for the both student network. Also
--   displays each network
theTest :: IO ()

-- | Solve the influence diagram <a>student</a> and convert it into a
--   policy network
policyTest :: IO ()

-- | Solve the <a>market</a> influence diagram
marketTest :: IO ()
instance Eq E
instance Enum E
instance Bounded E
instance Eq I
instance Enum I
instance Bounded I
instance Eq S
instance Enum S
instance Bounded S
instance Eq F
instance Enum F
instance Bounded F
instance Eq IN
instance Enum IN
instance Bounded IN
instance Eq EF
instance Enum EF
instance Bounded EF


-- | A comparison of influence diagram solution with references
module Bayes.Test.InfluencePatterns
testStudentDecisions :: IO ()


-- | Testing of the implementation.
module Bayes.Test

-- | Run all the tests
runTests :: IO ()


-- | Sampling
--   
--   Samplers for Bayesian network inferences
module Bayes.Sampling

-- | Sampler defining the behavior of a sampling algorithms (init value,
--   sample generation, how to select nodes in the grapg)
data Sampler g a
Sampler :: !b -> !GenIO -> IO (Sample g a) -> !GenIO -> SamplerGraph g a -> !SamplingScheme g b a -> Sampler g a

-- | A sample (graph of instantiations)
type Sample g a = BayesianNetwork g a

-- | Sample a bayesian network using a given sampling scheme
runSampling :: (DirectedGraph g, FunctorWithVertex g) => Int -> Int -> Sampler g a -> IO [Sample g a]

-- | Return the vertices in topological order
topologicalOrder :: DirectedGraph g => g a b -> [Vertex]

-- | Ancestral sampler which does not support evidence
discreteAncestralSampler :: (Factor f, FunctorWithVertex g, DirectedGraph g) => BayesianNetwork g f -> Sampler g DVI

-- | Gibbs sampling
gibbsSampler :: (Factor f, FunctorWithVertex g, DirectedGraph g) => BayesianNetwork g f -> [DVI] -> Sampler g DVI

-- | Gibbs sampling
gibbsMCMCSampler :: (Factor f, FunctorWithVertex g, DirectedGraph g) => BayesianNetwork g f -> [DVI] -> Sampler g DVI

-- | Generate a graph of sampling histogram for each variable So, for a
--   vertex v we have the posterior values p(v)
samplingHistograms :: (InstantiationValue i v, BayesianVariable i, FunctorWithVertex g, Graph g) => Int -> [Sample g i] -> Sample g [(Double, Double, Double)]

-- | Compute the histogram of values
histogram :: Int -> [Double] -> [(Double, Double, Double)]
type ContinuousNetwork = SBN Distri
type ContinuousSample = SBN CVI
data Distri
D :: !CV -> !DistributionF DirectedSG (Double, Double) CVI -> Distri

-- | Gibbs sampling for continuous network
continuousMCMCSampler :: ContinuousNetwork -> [CVI] -> Sampler DirectedSG CVI
instance SampleGeneration CV CVI (Double, Double) Double
instance SamplingBounds (Double, Double) Double
instance SampleGeneration DV DVI DV Int
instance SamplingBounds DV Int
instance Graph g => Show (Sample g [(Double, Double, Double)])
instance Graph g => Show (Sample g CVI)
instance Graph g => Show (Sample g [DVI])


-- | Example of sampling
--   
--   Two samplers are availables : the <a>discreteAncestralSampler</a> and
--   the <a>gibbsSampler</a>. Only the <a>gibbsSampler</a> can be used with
--   evidence.
--   
--   In this example, we have a very simple network.
--   
--   <pre>
--       simple :: ([<a>TDV</a> Bool],<a>SBN</a> <a>CPT</a>)
--       simple = <a>runBN</a> $ do 
--           a &lt;- <a>variable</a> "a" (<a>t</a> :: Bool)
--           b &lt;- <a>variable</a> "b" (<a>t</a> :: Bool) 
--   --        
--           <a>proba</a> a <a>~~</a> [0.4,0.6]
--           <a>cpt</a> b [a] <a>~~</a> [0.8,0.2,0.2,0.8]
--   --    
--           return [a,b]
--   </pre>
--   
--   This network is representing a sensor b. We observe the value of b and
--   we want to infer the value of a.
--   
--   We use the <a>gibbsSampler</a> for this with an initial period of 200
--   samples which are dropped. The <a>gibbsSampler</a> is generate a
--   stream of samples. From this stream, we need to compute a probability
--   distribution. For this, we use the <a>samplingHistograms</a> histogram
--   function which is generating a list : the probability values of each
--   vertex.
--   
--   <pre>
--   let (vars@[a,b],exampleG) = simple
--   n &lt;- <a>runSampling</a> 5000 200 (<a>gibbsSampler</a> exampleG [b <a>=:</a> True])
--   let h = <a>samplingHistograms</a> n
--   print $ h
--   </pre>
--   
--   Then, we compare this result with the exact one we get with a junction
--   tree.
--   
--   <pre>
--   let jt = <a>createJunctionTree</a> <a>nodeComparisonForTriangulation</a> exampleG
--       jt' = <a>changeEvidence</a> [b <a>=:</a> True] jt
--   mapM_ (x -&gt; print . <a>posterior</a> jt' $ [x]) vars
--   </pre>
--   
--   We can also use the <a>discreteAncestralSampler</a> to compute the
--   posterior but it is not supporting the use of evidence in this
--   version. The syntax is similar.
--   
--   <pre>
--   n &lt;- <a>runSampling</a> 500 (<a>discreteAncestralSampler</a> exampleG)
--   </pre>
module Bayes.Examples.Sampling
testSampling :: IO ()


-- | Expectation / Maximization to learn Bayesian network values
module Bayes.EM

-- | Learn network values from samples using the expectation / maximization
--   algorithm.
learnEM :: (FunctorWithVertex g, NamedGraph g, FoldableWithVertex g, DirectedGraph g) => [[DVI]] -> BayesianNetwork g CPT -> BayesianNetwork g CPT


-- | Test of learning
--   
--   In this example, two networks are used : <a>simple</a> which is the
--   reference and <a>wrong</a> which is a wrong start. The goal is to use
--   test patterns to learn the right <a>simple</a> network from
--   <a>wrong</a>. Only the values are learnt. The topology of both
--   networks is the same.
--   
--   <pre>
--   simple :: ([<a>TDV</a> Bool],<a>SBN</a> <a>CPT</a>)
--   simple = <a>runBN</a> $ do 
--       a &lt;- <a>variable</a> "a" (<a>t</a> :: Bool)
--       b &lt;- <a>variable</a> "b" (<a>t</a> :: Bool) 
--   --    
--       <a>proba</a> a <a>~~</a> [0.4,0.6]
--       <a>cpt</a> b [a] <a>~~</a> [0.8,0.2,0.2,0.8]
--   --
--       return [a,b]
--   </pre>
--   
--   and <a>wrong</a> where the probability for a is wrong.
--   
--   <pre>
--   wrong :: ([<a>TDV</a> Bool],<a>SBN</a> <a>CPT</a>)
--   wrong = <a>runBN</a> $ do 
--       a &lt;- <a>variable</a> "a" (<a>t</a> :: Bool)
--       b &lt;- <a>variable</a> "b" (<a>t</a> :: Bool) 
--   --    
--       <a>proba</a> a <a>~~</a> [0.2,0.8]
--       <a>cpt</a> b [a] <a>~~</a> [0.8,0.2,0.2,0.8]
--   --
--       return [a,b]
--   </pre>
--   
--   So, the first thing to do is generate test patterns. We are using the
--   <a>discreteAncestralSampler</a> for this. This function is generating
--   a sequence of graphs. We are just interested in the values. So, we get
--   the values with <a>allVertexValues</a>.
--   
--   <pre>
--   generatePatterns :: IO [[DVI]]
--   generatePatterns = do 
--       let (vars@[a,b],exampleG) = simple
--       r &lt;- <a>runSampling</a> 5000 0 (<a>discreteAncestralSampler</a> exampleG)
--       return (map <a>allVertexValues</a> r)
--   </pre>
--   
--   Once we have the data, we can try to learn the network:
--   
--   <pre>
--   emTest = do 
--     samples &lt;- generatePatterns 
--     let (_,simpleG) = simple 
--         (_,wrongG) = wrong 
--     print simpleG 
--     <a>printGraphValues</a> simpleG
--     <a>printGraphValues</a> wrongG
--   --
--     <a>printGraphValues</a> (<a>learnEM</a> samples wrongG)
--   </pre>
--   
--   First, we display the topology of the graph and the values for the
--   reference graph and the wrong one. Then, we use the <a>learnEM</a>
--   function to learn a new network from the samples. And, we print the
--   new values to check.
module Bayes.Examples.EMTest
emTest :: IO ()

module Bayes.Continuous

-- | The Bayesian monad
type CNMonad a = GraphMonad DirectedSG () Distri a

-- | A continuous variable
data CV

-- | An expression which can be a constant, variable or formula. In case it
--   is a variable, it can be used as a <a>BayesianVariable</a> or
--   instantiated as an <a>Instantiable</a> type. Otherwise you'll get an
--   error
data DN

-- | This class is used to simplify the network description. Variable names
--   can be optional. In that later case, () must be used instead of a
--   name.
class VariableName m
mkVariable :: VariableName m => m -> CNMonad CV

-- | A Bayesian Variable is a variable part of Bayesian network and so
--   which knows its position : the vertex.
class BayesianVariable v
vertex :: BayesianVariable v => v -> Vertex
data Distri
type ContinuousNetwork = SBN Distri
type ContinuousSample = SBN CVI
class InstantiationValue i v | i -> v
instantiationValue :: InstantiationValue i v => i -> v
toDouble :: InstantiationValue i v => i -> Double

-- | A continuous variable instantiation
data CVI

-- | Uniform dstribution
uniform :: VariableName s => s -> DN -> DN -> CNMonad DN

-- | Normal distribution
normal :: VariableName s => s -> DN -> DN -> CNMonad DN

-- | Beta distribution
beta :: VariableName s => s -> DN -> DN -> CNMonad DN

-- | Beta' distribution
beta' :: VariableName s => s -> DN -> DN -> CNMonad DN

-- | Exponential distribution
exponential :: VariableName s => s -> DN -> CNMonad DN

-- | Create a network but only returns the monad value. Mainly used for
--   testing.
execCN :: CNMonad a -> ContinuousNetwork

-- | Create a network using the simple graph implementation The initialized
--   nodes are replaced by the value. Returns the monad values and the
--   built graph.
runCN :: CNMonad a -> (a, ContinuousNetwork)

-- | Create a bayesian network but only returns the monad value. Mainly
--   used for testing.
evalCN :: CNMonad a -> a

-- | Sample a bayesian network using a given sampling scheme
runSampling :: (DirectedGraph g, FunctorWithVertex g) => Int -> Int -> Sampler g a -> IO [Sample g a]

-- | Gibbs sampling for continuous network
continuousMCMCSampler :: ContinuousNetwork -> [CVI] -> Sampler DirectedSG CVI

-- | Create a variable instantiation using values from an enumeration
(=:) :: Instantiable d v r => d -> v -> r

-- | Compute the histogram of values
histogram :: Int -> [Double] -> [(Double, Double, Double)]

-- | Generate a graph of sampling histogram for each variable So, for a
--   vertex v we have the posterior values p(v)
samplingHistograms :: (InstantiationValue i v, BayesianVariable i, FunctorWithVertex g, Graph g) => Int -> [Sample g i] -> Sample g [(Double, Double, Double)]
instance VariableName ()
instance VariableName String
instance Fractional DN
instance Num DN
instance Instantiable DN Double CVI
instance BayesianVariable DN
instance Fractional RN
instance Num RN
instance Monoid (DistributionSupport (Double, Double))


-- | Sampling example with continuous distributions
--   
--   Continuous networks can't be handled by any of the functions defined
--   for the discrete networks. So, instead of using exact inference
--   algorithms like the junction trees, sampling method have to be used.
--   
--   In this example, we want to estimate a parameter which is measured by
--   noisy sensors.
--   
--   There are <a>nbSensors</a> available. They are described with a
--   <a>normal</a> distribution centered on the value of the unknown
--   parameters and with a standard deviation of 0.1.
--   
--   The unknown parameter is described with a <a>uniform</a> distribution
--   bounded by 1.0 and 2.0.
--   
--   First, we describe the sensor:
--   
--   <pre>
--   sensor :: <a>DN</a> -&gt; <a>CNMonad</a> <a>DN</a> 
--   sensor p = do 
--       <a>normal</a> () p 0.1 
--   </pre>
--   
--   It is just a <a>normal</a> distribution. The mean of this distribution
--   is the parameters p. This parameter has special type <a>DN</a>. All
--   expressions used to build the continuous bayesian network are using
--   values of type <a>DN</a>. A value of type <a>DN</a> can either
--   represent a constant, a variable or an expression.
--   
--   If the sensor was biased, we could write:
--   
--   <pre>
--   <a>normal</a> ()  (p + 0.2) 0.1
--   </pre>
--   
--   The Bayesian network describing the measurement process is given by:
--   
--   <pre>
--   test = <a>runCN</a> $ do
--     a &lt;- <a>uniform</a> "a" 1.0 2.0 -- Unknown parameter
--     sensors &lt;- sequence (replicate <a>nbSensors</a> (sensor a))
--     return (a:sensors)
--   </pre>
--   
--   We are connecting <a>nbSensors</a> nodes corresponding to the
--   <a>nbSensors</a> measurements. In real life it can either be different
--   sensors or the same one used several times (assuming the value of the
--   parameter is not dependent on time).
--   
--   Now, as usual in all the examples of this package, we get the bayesian
--   graph and a list of variables used to compute some posterior or define
--   some evidences
--   
--   <pre>
--   debugcn = do 
--       let ((a:sensors), testG) = test
--   </pre>
--   
--   Then, we generate some random measurements and create the evidences
--   
--   <pre>
--   g &lt;- create 
--   measurements &lt;- sequence . replicate  nbSensors $ (MWC.normal 1.5 0.1 g)
--   let evidence = zipWith (=:) sensors measurements
--   </pre>
--   
--   Evidence has type <a>CVI</a> and is created with the assigment
--   operator <a>=:</a> .
--   
--   Now, we generate some samples to estimate the posterior distributions.
--   
--   <pre>
--   n &lt;- <a>runSampling</a> 10000 200 (<a>continuousMCMCSampler</a> testG evidence)
--   </pre>
--   
--   This function is generating a sequence of graphs ! We are not
--   interested in the sensor values. They are known and fixed since they
--   have been measured. So, we extract the value of the parameter.
--   
--   <pre>
--   let samples = map (g -&gt; <a>instantiationValue</a> . fromJust . <a>vertexValue</a> g $ (<a>vertex</a> a)) n
--   </pre>
--   
--   And with the samples for the parameters we can compute an histogram
--   and get an approximation of the posterior.
--   
--   <pre>
--   let samples = map (g -&gt; <a>instantiationValue</a> . fromJust . <a>vertexValue</a> g $ (<a>vertex</a> a)) n
--       h = <a>histogram</a> 6 samples 
--   print h
--   </pre>
--   
--   We see in the histogram that the estimated value is around 1.5.
module Bayes.Examples.ContinuousSampling
nbSensors :: Int
sensor :: DN -> CNMonad DN
test :: ([DN], ContinuousNetwork)
debugcn :: IO ()
