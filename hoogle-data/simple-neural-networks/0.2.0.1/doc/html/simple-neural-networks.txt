-- Hoogle documentation, generated by Haddock
-- See Hoogle, http://www.haskell.org/hoogle/


-- | Simple parallel neural networks implementation
--   
--   Simple parallel neural networks implementation
@package simple-neural-networks
@version 0.2.0.1


-- | Simple parallel neural networks implementation
--   
--   <pre>
--     import AI.NeuralNetworks.Simple
--     import Text.Printf
--     import System.Random
--     import Control.Monad
--   
--   calcXor net x y =
--       let [r] = runNeuralNetwork net [x, y]
--       in  r
--   
--   mse net =
--       let square x = x * x
--           e1 = square $ calcXor net 0 0
--           e2 = square $ calcXor net 1 0 - 1
--           e3 = square $ calcXor net 0 1 - 1
--           e4 = square $ calcXor net 1 1
--       in 0.5 * (e1 + e2 + e3 + e4)
--   
--   stopf best gnum = do
--       let e = mse best
--       when (gnum `rem` 100 == 0) $
--         printf "Generation: %02d, MSE: %.4f\n" gnum e
--       return $ e &lt; 0.002 || gnum &gt;= 10000
--   
--   main = do
--       gen &lt;- newStdGen
--       let (randomNet, _) = randomNeuralNetwork gen [2,2,1] [Logistic, Logistic] 0.45
--           examples = [ ([0,0],[0]), ([0,1],[1]), ([1,0],[1]), ([1,1],[0]) ]
--       net &lt;- backpropagationBatchParallel randomNet examples 0.4 stopf :: IO (NeuralNetwork Double)
--       putStrLn ""
--       putStrLn $ "Result: " ++ show net
--       _ &lt;- printf "0 xor 0 = %.4f\n" (calcXor net 0 0)
--       _ &lt;- printf "1 xor 0 = %.4f\n" (calcXor net 1 0)
--       _ &lt;- printf "0 xor 1 = %.4f\n" (calcXor net 0 1)
--       printf "1 xor 1 = %.4f" (calcXor net 1 1)
--   </pre>
module AI.NeuralNetworks.Simple

-- | Activation function
data ActivationFunction

-- | Hyperbolic tangent
Tanh :: ActivationFunction

-- | Logistic function : 1 / (1 + exp (-x))
Logistic :: ActivationFunction

-- | Neural network
data NeuralNetwork a

-- | Deltas calculated by backpropagation algorithm
data WeightDeltas a

-- | Neural network with all weights set to zero.
--   
--   <pre>
--   {- 
--      2 input neurons,
--      one hidden layer with 2 neurons and tanh activation function,
--      one output layer with 1 neuron and tanh activation function
--   -}
--   emptyNeuralNetwork [2, 2, 1] [Tanh, Tanh]
--   </pre>
emptyNeuralNetwork :: [Word16] -> [ActivationFunction] -> NeuralNetwork a

-- | Weights of the given neural network.
getWeights :: NeuralNetwork a -> [((Word16, Word16, Word16), a)]

-- | Change weights of the given neural network.
setWeights :: [((Word16, Word16, Word16), a)] -> NeuralNetwork a -> NeuralNetwork a

-- | Run neural network.
runNeuralNetwork :: (Num a, Floating a) => NeuralNetwork a -> [a] -> [a]

-- | Run one step of the backpropagation algorithm.
backpropagationOneStep :: (Num a, Floating a) => NeuralNetwork a -> a -> [a] -> [a] -> WeightDeltas a

-- | Run backpropagation algorithm in stochastic mode.
backpropagationStochastic :: (Num a, Floating a) => NeuralNetwork a -> [([a], [a])] -> a -> (NeuralNetwork a -> Int -> IO Bool) -> IO (NeuralNetwork a)

-- | Run backpropagation algorithm in batch mode. This code runs faster in
--   parallel, so don't forget to use +RTS -N.
backpropagationBatchParallel :: (Num a, Floating a, NFData a) => NeuralNetwork a -> [([a], [a])] -> a -> (NeuralNetwork a -> Int -> IO Bool) -> IO (NeuralNetwork a)

-- | Apply deltas to the neural netwotk.
applyWeightDeltas :: (Num a, Floating a) => WeightDeltas a -> NeuralNetwork a -> NeuralNetwork a

-- | Union list of deltas into one WeightDeltas.
unionWeightDeltas :: (Num a, Floating a) => [WeightDeltas a] -> WeightDeltas a

-- | Generate random neural network.
randomNeuralNetwork :: (RandomGen g, Random a, Num a, Ord a) => g -> [Word16] -> [ActivationFunction] -> a -> (NeuralNetwork a, g)

-- | Crossover of two neural networks.
crossoverCommon :: (Num a, RandomGen g) => g -> NeuralNetwork a -> NeuralNetwork a -> ([NeuralNetwork a], g)

-- | Another implementation of crossover. Weights of a child are just some
--   function of corresponding parent weights.
crossoverMerge :: (Num a, RandomGen g) => (a -> a -> a) -> g -> NeuralNetwork a -> NeuralNetwork a -> ([NeuralNetwork a], g)

-- | Mutate given neural netwrok.
mutationCommon :: (Random a, Num a, RandomGen g) => Double -> a -> g -> NeuralNetwork a -> (NeuralNetwork a, g)
instance Show ActivationFunction
instance Read ActivationFunction
instance Eq ActivationFunction
instance Show a => Show (NeuralNetwork a)
instance Read a => Read (NeuralNetwork a)
instance Eq a => Eq (NeuralNetwork a)
instance Show a => Show (WeightDeltas a)
instance Read a => Read (WeightDeltas a)
instance Eq a => Eq (WeightDeltas a)
instance NFData a => NFData (WeightDeltas a)
instance NFData a => NFData (NeuralNetwork a)
