-- Hoogle documentation, generated by Haddock
-- See Hoogle, http://www.haskell.org/hoogle/


-- | Hidden Markov Models using HMatrix primitives
--   
@package hmm-hmatrix
@version 0.0.1

module Math.HiddenMarkovModel.Distribution
newtype State
State :: Int -> State
class (Container Vector (Probability distr), Product (Probability distr)) => Info distr
numberOfStates :: Info distr => distr -> Int
class (Container Vector (Probability distr), Product (Probability distr)) => Generate distr
generate :: (Generate distr, RandomGen g, Probability distr ~ prob, Emission distr ~ emission) => distr -> State -> State g emission
class (Container Vector (Probability distr), Product (Probability distr)) => EmissionProb distr
emissionProb :: EmissionProb distr => distr -> Emission distr -> Vector (Probability distr)
class (EmissionProb (Distribution tdistr), Trained (Distribution tdistr) ~ tdistr) => Estimate tdistr where type family Distribution tdistr
accumulateEmissions :: (Estimate tdistr, Distribution tdistr ~ distr, Probability distr ~ prob) => [[(Emission distr, prob)]] -> tdistr
combine :: Estimate tdistr => tdistr -> tdistr -> tdistr
normalize :: (Estimate tdistr, Distribution tdistr ~ distr) => tdistr -> distr
newtype Discrete prob symbol
Discrete :: (Map symbol (Vector prob)) -> Discrete prob symbol
newtype DiscreteTrained prob symbol
DiscreteTrained :: (Map symbol (Vector prob)) -> DiscreteTrained prob symbol
newtype Gaussian a
Gaussian :: (Array State (Vector a, Matrix a, a)) -> Gaussian a
newtype GaussianTrained a
GaussianTrained :: (Map State (Vector a, Matrix a, a)) -> GaussianTrained a
gaussian :: Field prob => [(Vector prob, Matrix prob)] -> Gaussian prob
class CSV distr
toCells :: CSV distr => distr -> [[String]]
parseCells :: CSV distr => Int -> CSVParser distr
type CSVParser = StateT CSVResult (Exceptional String)
class Ord symbol => CSVSymbol symbol
cellFromSymbol :: CSVSymbol symbol => symbol -> String
symbolFromCell :: CSVSymbol symbol => String -> Maybe symbol
instance Eq State
instance Ord State
instance Show State
instance Read State
instance Ix State
instance (Show prob, Show symbol, Storable prob) => Show (Discrete prob symbol)
instance (Show prob, Show symbol, Storable prob) => Show (DiscreteTrained prob symbol)
instance (Show a, Element a) => Show (Gaussian a)
instance (Show a, Element a) => Show (GaussianTrained a)
instance (Field a, Eq a, Show a, Read a) => CSV (Gaussian a)
instance (Field prob, Show prob, Read prob, CSVSymbol symbol) => CSV (Discrete prob symbol)
instance CSVSymbol Int
instance CSVSymbol Char
instance (Numeric a, Field a) => Estimate (GaussianTrained a)
instance (Numeric a, Field a) => EmissionProb (Gaussian a)
instance (Field a, Ord a, Random a) => Generate (Gaussian a)
instance Field a => Info (Gaussian a)
instance (Container Vector prob, Product prob, Ord symbol) => Estimate (DiscreteTrained prob symbol)
instance (Container Vector prob, Product prob, Ord symbol) => EmissionProb (Discrete prob symbol)
instance (Container Vector prob, Product prob, Ord symbol, Ord prob, Random prob) => Generate (Discrete prob symbol)
instance (Container Vector prob, Product prob, Ord symbol) => Info (Discrete prob symbol)
instance Enum State

module Math.HiddenMarkovModel.Named

-- | A Hidden Markov Model with names for each state.
--   
--   Although <a>nameFromStateMap</a> and <a>stateFromNameMap</a> are
--   exported you must be careful to keep them consistent when you alter
--   them.
data T distr prob
Cons :: T distr prob -> Map State String -> Map String State -> T distr prob
model :: T distr prob -> T distr prob
nameFromStateMap :: T distr prob -> Map State String
stateFromNameMap :: T distr prob -> Map String State
type Discrete prob symbol = T (Discrete prob symbol) prob
type Gaussian a = T (Gaussian a) a
fromModelAndNames :: T distr prob -> [String] -> T distr prob
toCSV :: (CSV distr, Field prob, Show prob) => T distr prob -> String
fromCSV :: (CSV distr, Field prob, Read prob) => String -> Exceptional String (T distr prob)
instance (Show distr, Show prob, Element prob) => Show (T distr prob)
instance (Read distr, Read prob, Element prob) => Read (T distr prob)

module Math.HiddenMarkovModel

-- | A Hidden Markov model consists of a number of (hidden) states and a
--   set of emissions. There is a vector for the initial probability of
--   each state and a matrix containing the probability for switching from
--   one state to another one. The <a>distribution</a> field points to
--   probability distributions that associate every state with emissions of
--   different probability. Famous distribution instances are discrete and
--   Gaussian distributions. See <a>Math.HiddenMarkovModel.Distribution</a>
--   for details.
--   
--   The transition matrix is transposed with respect to popular HMM
--   descriptions. But I think this is the natural orientation, because
--   this way you can write "transition matrix times probability column
--   vector".
--   
--   The type has two type parameters, although the one for the
--   distribution would be enough. However, replacing <tt>prob</tt> by
--   <tt>Distr.Probability distr</tt> would prohibit the derived Show and
--   Read instances.
data T distr prob
Cons :: Vector prob -> Matrix prob -> distr -> T distr prob
initial :: T distr prob -> Vector prob
transition :: T distr prob -> Matrix prob
distribution :: T distr prob -> distr
data State
state :: Int -> State
type Discrete prob symbol = T (Discrete prob symbol) prob
type DiscreteTrained prob symbol = Trained (DiscreteTrained prob symbol) prob
type Gaussian a = T (Gaussian a) a
type GaussianTrained a = Trained (GaussianTrained a) a

-- | Create a model with uniform probabilities for initial vector and
--   transition matrix given a distribution for the emissions. You can use
--   this as a starting point for <a>trainUnsupervised</a>.
uniform :: (Info distr, Probability distr ~ prob) => distr -> T distr prob
generate :: (RandomGen g, Ord prob, Random prob, Generate distr, Probability distr ~ prob, Emission distr ~ emission) => T distr prob -> g -> [emission]

-- | Logarithm of the likelihood to observe the given sequence. We return
--   the logarithm because the likelihood can be so small that it may be
--   rounded to zero in the choosen number type.
logLikelihood :: (EmissionProb distr, Floating prob, Probability distr ~ prob, Emission distr ~ emission, Traversable f) => T distr prob -> T f emission -> prob

-- | Reveal the state sequence that led most likely to the observed
--   sequence of emissions. It is found using the Viterbi algorithm.
reveal :: (EmissionProb distr, Probability distr ~ prob, Emission distr ~ emission, Traversable f, Reverse f) => T distr prob -> T f emission -> T f State

-- | A trained model is a temporary form of a Hidden Markov model that we
--   need during the training on multiple training sequences. It allows to
--   collect knowledge over many sequences with <a>mergeTrained</a>, even
--   with mixed supervised and unsupervised training. You finish the
--   training by converting the trained model back to a plain modul using
--   <tt>finishTraining</tt>.
--   
--   You can create a trained model in three ways:
--   
--   <ul>
--   <li>supervised training using an emission sequence with associated
--   states,</li>
--   <li>unsupervised training using an emission sequence and an existing
--   Hidden Markov Model,</li>
--   <li>derive it from state sequence patterns, cf.
--   <a>Math.HiddenMarkovModel.Pattern</a>.</li>
--   </ul>
data Trained distr prob
Trained :: Vector prob -> Matrix prob -> distr -> Trained distr prob
trainedInitial :: Trained distr prob -> Vector prob
trainedTransition :: Trained distr prob -> Matrix prob
trainedDistribution :: Trained distr prob -> distr

-- | Contribute a manually labeled emission sequence to a HMM training.
trainSupervised :: (Estimate tdistr, Distribution tdistr ~ distr, Probability distr ~ prob, Emission distr ~ emission) => Int -> T [] (State, emission) -> Trained tdistr prob

-- | Consider a superposition of all possible state sequences weighted by
--   the likelihood to produce the observed emission sequence. Now train
--   the model with respect to all of these sequences with respect to the
--   weights. This is done by the Baum-Welch algorithm.
trainUnsupervised :: (Estimate tdistr, Distribution tdistr ~ distr, Probability distr ~ prob, Emission distr ~ emission) => T distr prob -> T [] emission -> Trained tdistr prob
mergeTrained :: (Estimate tdistr, Distribution tdistr ~ distr, Probability distr ~ prob, Emission distr ~ emission) => Trained tdistr prob -> Trained tdistr prob -> Trained tdistr prob
finishTraining :: (Estimate tdistr, Distribution tdistr ~ distr, Probability distr ~ prob, Emission distr ~ emission) => Trained tdistr prob -> T distr prob
trainMany :: (Estimate tdistr, Distribution tdistr ~ distr, Probability distr ~ prob, Foldable f) => (trainingData -> Trained tdistr prob) -> T f trainingData -> T distr prob

-- | Compute maximum deviation between initial and transition
--   probabilities. You can use this as abort criterion for unsupervised
--   training. We omit computation of differences between the emission
--   probabilities. This simplifies matters a lot and should suffice for
--   defining an abort criterion.
deviation :: (Field prob, Ord prob) => T distr prob -> T distr prob -> prob
toCSV :: (CSV distr, Field prob, Show prob) => T distr prob -> String
fromCSV :: (CSV distr, Field prob, Read prob) => String -> Exceptional String (T distr prob)


-- | This module provides a simple way to train the transition matrix and
--   initial probability vector using simple patterns of state sequences.
--   
--   You may create a trained model using semigroup combinators like this:
--   
--   <pre>
--   let a = atom $ HMM.state 0
--       b = atom $ HMM.state 1
--       distr =
--          Distr.DiscreteTrained $ Map.fromList $
--          ('a', Vector.fromList [1,2]) :
--          ('b', Vector.fromList [4,3]) :
--          ('c', Vector.fromList [0,1]) :
--          []
--   in  finish 2 distr $ replicate 5 $ replicate 10 a &lt;&gt; replicate 20 b
--   </pre>
module Math.HiddenMarkovModel.Pattern
data T prob
atom :: Container Vector prob => State -> T prob
append :: Container Vector prob => T prob -> T prob -> T prob
replicate :: Container Vector prob => Int -> T prob -> T prob
finish :: Container Vector prob => Int -> tdistr -> T prob -> Trained tdistr prob
instance Field prob => Semigroup (T prob)


-- | This is an example of an HMM with discrete emissions. We model a
--   traffic light consisting of the colors red, yellow, green, where only
--   one lamp can be switched on at every point in time. This way, when it
--   is yellow you cannot tell immediately whether it will switch to green
--   or red. We can only infer this from the light seen before.
--   
--   There are four hidden states: 0 emits red, 1 emits yellow between red
--   and green, 2 emits green, 3 emits yellow between green and red.
--   
--   We quantise time in time steps. The transition matrix of the model
--   <a>hmm</a> encodes the expected duration of every state counted in
--   time steps and what states follow after each other. E.g. transition
--   probability of 0.8 of a state to itself means that the expected
--   duration of the state is 5 time steps (1/(1-0.8)). However, it is a
--   geometric distribution, that is, shorter durations are always more
--   probable.
--   
--   The distribution of <a>hmm</a> encodes which lights a state activates.
--   In our case everything is deterministic: Every state can switch
--   exactly one light on.
--   
--   Given a sequence of observed lights the function <a>reveal</a> tells
--   us the most likely sequence of states. We test this with the light
--   sequences in <a>stateSequences</a> where we already know the hidden
--   states as they are stored in <a>labeledSequences</a>.
--   <a>verifyRevelation</a> compares the computed state sequence with the
--   given one.
--   
--   We also try some trainings in <a>hmmTrainedSupervised</a> et.al.

-- | <i>Warning: do not import that module, it is only intended for
--   demonstration</i>
module Math.HiddenMarkovModel.Example.TrafficLight
data Color
Red :: Color
Yellow :: Color
Green :: Color

-- | Using <a>show</a> and <a>read</a> is not always a good choice since
--   they must format and parse Haskell expressions which is not of much
--   use to the outside world.
hmm :: Discrete Double Color
hmmDisturbed :: Discrete Double Color
red :: (State, Color)
yellowGR :: (State, Color)
green :: (State, Color)
yellowRG :: (State, Color)
labeledSequences :: T [] (T [] (State, Color))

-- | Construct a Hidden Markov model by watching a set of manually created
--   sequences of emissions and according states.
hmmTrainedSupervised :: Discrete Double Color
stateSequences :: T [] (T [] Color)

-- | Construct a Hidden Markov model starting from a known model and a set
--   of sequences that contain only the emissions, but no states.
hmmTrainedUnsupervised :: Discrete Double Color

-- | Repeat unsupervised training until convergence.
hmmIterativelyTrained :: Discrete Double Color
verifyRevelation :: Discrete Double Color -> T [] (State, Color) -> Bool
verifyRevelations :: [Bool]
instance Eq Color
instance Ord Color
instance Enum Color
instance Show Color
instance Read Color
instance CSVSymbol Color


-- | Example of an HMM with continuous emissions. We train a model to
--   accept sine waves of a certain frequency.
--   
--   There are four hidden states: 0 - rising, 1 - high, 2 - falling, 3 -
--   low.

-- | <i>Warning: do not import that module, it is only intended for
--   demonstration</i>
module Math.HiddenMarkovModel.Example.SineWave
hmm :: Gaussian Double
sineWaveLabeled :: T [] (State, Double)
sineWave :: T [] Double
revealed :: T [] State
hmmTrainedSupervised :: Gaussian Double
hmmTrainedUnsupervised :: Gaussian Double
hmmIterativelyTrained :: Gaussian Double


-- | Example of an HMM with continuous emissions with two-dimensional
--   observations. We train a model to accept a parametric curve of a
--   circle with a certain speed. This is like
--   <a>Math.HiddenMarkovModel.Example.SineWave</a> but in two dimensions.
--   
--   The four hidden states correspond to the four quadrants.

-- | <i>Warning: do not import that module, it is only intended for
--   demonstration</i>
module Math.HiddenMarkovModel.Example.Circle
hmm :: Gaussian Double
circleLabeled :: T [] (State, Vector Double)
circle :: T [] (Vector Double)
revealed :: T [] State

-- | Sample multivariate normal distribution and reconstruct it from the
--   samples. You should obtain the same parameters.
reconstructDistribution :: Gaussian Double
hmmTrainedSupervised :: Gaussian Double
hmmTrainedUnsupervised :: Gaussian Double
hmmIterativelyTrained :: Gaussian Double
