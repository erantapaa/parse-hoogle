-- Hoogle documentation, generated by Haddock
-- See Hoogle, http://www.haskell.org/hoogle/


-- | Online sampler for Latent Dirichlet Allocation
--   
--   Online Gibbs sampler for Latent Dirichlet Allocation. LDA is a
--   generative admixture model frequently used for topic modeling and
--   other applications. The primary goal of this implementation is to be
--   used for probabilistic soft word class induction. The sampler can be
--   used in an online as well as batch mode. This package uses an
--   imperative implementation in the ST monad.
@package swift-lda
@version 0.4.1

module NLP.SwiftLDA.UnboxedMaybeVector
instance (Num a, Unbox a) => Vector Vector (Maybe a)
instance (Num a, Unbox a) => MVector MVector (Maybe a)
instance (Num a, Unbox a) => Unbox (Maybe a)


-- | Latent Dirichlet Allocation
--   
--   Imperative implementation of a collapsed Gibbs sampler for LDA. This
--   library uses the topic modeling terminology (documents, words,
--   topics), even though it is generic. For example if used for word class
--   induction, replace documents with word types, words with features and
--   topics with word classes.
module NLP.SwiftLDA

-- | <tt>pass batch</tt> runs one pass of Gibbs sampling on documents in
--   <tt>batch</tt>
pass :: Int -> LDA s -> Vector Doc -> ST s (Vector Doc)

-- | Run a pass on a single doc
passOne :: Int -> LDA s -> Doc -> ST s Doc

-- | Abstract type holding the settings and the state of the sampler
data LDA s
type Doc = (D, Vector (W, Maybe Z))
type D = Int
type W = Int
type Z = Int
type Table2D = IntMap Table1D
type Table1D = IntMap Double
data Finalized
Finalized :: !Table2D -> !Table2D -> !Table1D -> !Table2D -> !Table2D -> !Double -> !Double -> !Int -> !Int -> !Maybe Double -> Finalized

-- | Document topic counts
docTopics :: Finalized -> !Table2D

-- | Word topic counts
wordTopics :: Finalized -> !Table2D

-- | Topics counts
topics :: Finalized -> !Table1D

-- | Inverse document-topic counts
topicDocs :: Finalized -> !Table2D

-- | Inverse word-topic counts
topicWords :: Finalized -> !Table2D

-- | alpha * K Dirichlet parameter (topic sparseness)
alphasum :: Finalized -> !Double

-- | beta Dirichlet parameter (word sparseness)
beta :: Finalized -> !Double

-- | Number of topics K
topicNum :: Finalized -> !Int

-- | Number of unique words
wSize :: Finalized -> !Int

-- | Learning rate exponent
exponent :: Finalized -> !Maybe Double

-- | <tt>initial s k a b</tt> initializes model with <tt>k</tt> topics,
--   <tt>a/k</tt> alpha hyperparameter, <tt>b</tt> beta hyperparameter and
--   random seed <tt>s</tt>
initial :: Vector Word32 -> Int -> Double -> Double -> Maybe Double -> ST s (LDA s)

-- | Create transparent immutable object holding model information from
--   opaque internal representation
finalize :: LDA s -> ST s Finalized
docTopicWeights_ :: LDA s -> Doc -> ST s (Vector Double)
priorDocTopicWeights_ :: LDA s -> D -> ST s (Vector Double)

-- | <tt>docTopicWeights m doc</tt> returns unnormalized topic
--   probabilities for document doc given LDA model <tt>m</tt>
docTopicWeights :: Finalized -> Doc -> Vector Double

-- | <tt>topicWeights m d w</tt> returns the unnormalized probabilities of
--   topics for word <tt>w</tt> in document <tt>d</tt> given LDA model
--   <tt>m</tt>.
wordTopicWeights :: Finalized -> D -> W -> Vector Double

-- | For each document sum the topic counts
docCounts :: Finalized -> Table1D
instance Generic Finalized
instance Datatype D1Finalized
instance Constructor C1_0Finalized
instance Selector S1_0_0Finalized
instance Selector S1_0_1Finalized
instance Selector S1_0_2Finalized
instance Selector S1_0_3Finalized
instance Selector S1_0_4Finalized
instance Selector S1_0_5Finalized
instance Selector S1_0_6Finalized
instance Selector S1_0_7Finalized
instance Selector S1_0_8Finalized
instance Selector S1_0_9Finalized
