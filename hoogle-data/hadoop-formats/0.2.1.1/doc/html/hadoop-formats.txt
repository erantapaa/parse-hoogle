-- Hoogle documentation, generated by Haddock
-- See Hoogle, http://www.haskell.org/hoogle/


-- | Read/write file formats commonly used by Hadoop.
--   
--   Currently this package only supports reading sequence files.
--   
--   You will need to have <a>libsnappy</a> installed to build this
--   project. If you are using OSX and homebrew to install snappy then the
--   following should get everything installed successfully.
--   
--   <pre>
--   $ brew install snappy
--   $ SNAPPY=$(brew --prefix snappy)
--   $ export C_INCLUDE_PATH=$SNAPPY/include
--   $ export LIBRARY_PATH=$SNAPPY/lib
--   $ cabal install hadoop-formats
--   </pre>
@package hadoop-formats
@version 0.2.1.1

module Data.Hadoop.Writable

-- | Equivalent to the java interface <i>org.apache.hadoop.io.Writable</i>.
--   All serializable <i>key</i> or <i>value</i> types in the Hadoop
--   Map-Reduce framework implement this interface.
class Collection a ~ c a => Writable c a
javaType :: Writable c a => a -> Text
decoder :: Writable c a => Decoder (c a)

-- | A specialized decoder for different types of writable.
data Decoder a

-- | The slowest. Variable length data.
Variable :: (ByteString -> Vector Int -> a) -> Decoder a

-- | All values are 16-bit little endian.
LE16 :: (ByteString -> a) -> Decoder a

-- | All values are 32-bit little endian.
LE32 :: (ByteString -> a) -> Decoder a

-- | All values are 64-bit little endian.
LE64 :: (ByteString -> a) -> Decoder a

-- | All values are 16-bit big endian.
BE16 :: (ByteString -> a) -> Decoder a

-- | All values are 32-bit big endian.
BE32 :: (ByteString -> a) -> Decoder a

-- | All values are 64-bit big endian.
BE64 :: (ByteString -> a) -> Decoder a
split :: Vector v a => (ByteString -> a) -> ByteString -> Vector Int -> v a
vintSize :: Word8 -> Int
bytesToVector :: (Storable a, Unbox a) => ByteString -> Vector a
instance Functor Decoder
instance Writable Vector Text
instance Writable Vector ByteString
instance Writable Vector Double
instance Writable Vector Float
instance Writable Vector Int64
instance Writable Vector Int32
instance Writable Vector Int16
instance Writable Vector ()

module Data.Hadoop.SequenceFile.Types

-- | The header of a sequence file. Contains the names of the Java classes
--   used to encode the file and potentially some metadata.
data Header
Header :: !Text -> !Text -> !Text -> ![(Text, Text)] -> !MD5 -> Header

-- | Package qualified class name of the key type.
hdKeyType :: Header -> !Text

-- | Package qualified class name of the value type.
hdValueType :: Header -> !Text

-- | Package qualified class name of the compression codec.
hdCompressionType :: Header -> !Text

-- | File metadata.
hdMetadata :: Header -> ![(Text, Text)]

-- | The synchronization pattern used to check for corruption throughout
--   the file.
hdSync :: Header -> !MD5

-- | An MD5 hash. Stored between each record block in a sequence file to
--   check for corruption.
newtype MD5
MD5 :: ByteString -> MD5
unMD5 :: MD5 -> ByteString

-- | A block of key/value pairs. The key at index <i>i</i> always relates
--   to the value at index <i>i</i>. Both vectors will always be the same
--   size.
data RecordBlock k v
RecordBlock :: Int -> Collection k -> Collection v -> RecordBlock k v

-- | The number of records.
rbCount :: RecordBlock k v -> Int

-- | The keys.
rbKeys :: RecordBlock k v -> Collection k

-- | The values.
rbValues :: RecordBlock k v -> Collection v
instance Eq MD5
instance Ord MD5
instance Eq Header
instance Ord Header
instance Show Header
instance Show MD5

module Data.Hadoop.SequenceFile.Parser

-- | Attoparsec <a>Parser</a> for sequence file headers.
header :: Parser Header

-- | Attoparsec <a>Parser</a> for sequence file record blocks.
recordBlock :: (Writable ck k, Writable cv v) => Header -> Parser (RecordBlock k v)


-- | This module allows for lazy decoding of hadoop sequence files from a
--   lazy <a>ByteString</a>. In the future an incremental API using strict
--   <a>ByteString</a> will be provided, but for now if you need that level
--   of control you need to use the attoparsec parsers in
--   <a>Data.Hadoop.SequenceFile.Parser</a> directly.
--   
--   <b>Basic Examples</b>
--   
--   <pre>
--   import           Control.Applicative ((&lt;$&gt;))
--   import qualified Data.ByteString.Lazy as L
--   import qualified Data.Foldable as F
--   import           Data.Int (Int32)
--   import           Data.Text (Text)
--   import qualified Data.Text.IO as T
--   
--   import           Data.Hadoop.SequenceFile
--   
--   -- | Print all the keys in a sequence file.
--   printKeys :: FilePath -&gt; IO ()
--   printKeys path = do
--       bs &lt;- L.readFile path
--       let records = decode bs :: Stream (RecordBlock Text Int32)
--       F.for_ records $ \rb -&gt; do
--           F.mapM_ T.putStrLn (rbKeys rb)
--   
--   -- | Count the number of records in a sequence file.
--   recordCount :: FilePath -&gt; IO ()
--   recordCount path = do
--       bs &lt;- L.readFile path
--       let records = decode bs :: Stream (RecordBlock Text Int32)
--       print $ F.sum $ rbCount &lt;$&gt; records
--   </pre>
--   
--   <b>Integration with Conduit</b>
--   
--   <pre>
--   sourceRecords :: MonadIO m =&gt; FilePath -&gt; Source m (RecordBlock Text ByteString)
--   sourceRecords path = do
--       bs &lt;- liftIO (L.readFile path)
--       F.traverse_ yield (decode bs)
--   </pre>
module Data.Hadoop.SequenceFile

-- | A lazy stream of values.
data Stream a
Error :: !String -> Stream a
Value :: !a -> (Stream a) -> Stream a
Done :: Stream a

-- | Equivalent to the java interface <i>org.apache.hadoop.io.Writable</i>.
--   All serializable <i>key</i> or <i>value</i> types in the Hadoop
--   Map-Reduce framework implement this interface.
class Collection a ~ c a => Writable c a
javaType :: Writable c a => a -> Text
decoder :: Writable c a => Decoder (c a)

-- | A block of key/value pairs. The key at index <i>i</i> always relates
--   to the value at index <i>i</i>. Both vectors will always be the same
--   size.
data RecordBlock k v
RecordBlock :: Int -> Collection k -> Collection v -> RecordBlock k v

-- | The number of records.
rbCount :: RecordBlock k v -> Int

-- | The keys.
rbKeys :: RecordBlock k v -> Collection k

-- | The values.
rbValues :: RecordBlock k v -> Collection v

-- | Decode a lazy <a>ByteString</a> in to a stream of record blocks.
decode :: (Writable ck k, Writable cv v) => ByteString -> Stream (RecordBlock k v)
instance Eq a => Eq (Stream a)
instance Ord a => Ord (Stream a)
instance Show a => Show (Stream a)
instance Foldable Stream
instance Functor Stream
