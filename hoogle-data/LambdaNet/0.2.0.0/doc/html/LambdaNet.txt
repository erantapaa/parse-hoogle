-- Hoogle documentation, generated by Haddock
-- See Hoogle, http://www.haskell.org/hoogle/


-- | A configurable and extensible neural network library
--   
--   LambdaNet is an artificial neural network library that allows users to
--   compose their own networks from function primitives.
--   
--   Documentation and nightly builds for LambdaNet can be found at
--   (<a>http://github.com/jbarrow/LambdaNet</a>).
@package LambdaNet
@version 0.2.0.0

module Network.Neuron

-- | Using this structure allows users of the library to create their own
--   neurons by creating two functions - an activation function and its
--   derivative - and packaging them up into a neuron type.
data Neuron a
Neuron :: (ActivationFunction a) -> (ActivationFunction' a) -> Neuron a
activation :: Neuron a -> (ActivationFunction a)
activation' :: Neuron a -> (ActivationFunction' a)
type ActivationFunction a = a -> a
type ActivationFunction' a = a -> a

-- | Our provided neuron types: sigmoid, tanh, reclu
sigmoidNeuron :: Floating a => Neuron a
tanhNeuron :: Floating a => Neuron a
recluNeuron :: Floating a => Neuron a

-- | The sigmoid activation function, a standard activation function
--   defined on the range (0, 1).
sigmoid :: Floating a => a -> a

-- | The derivative of the sigmoid function conveniently can be computed in
--   terms of the sigmoid function.
sigmoid' :: Floating a => a -> a
tanh :: Floating a => a -> a

-- | The hyperbolic tangent activation function is provided in Prelude.
--   Here we provide the derivative. As with the sigmoid function, the
--   derivative of tanh can be computed in terms of tanh.
tanh' :: Floating a => a -> a

-- | The rectified linear activation function. This is a more "biologically
--   accurate" activation function that still retains differentiability.
reclu :: Floating a => a -> a

-- | The derivative of the rectified linear activation function is just the
--   sigmoid.
reclu' :: Floating a => a -> a

module Network.Layer

-- | The LayerDefinition type is an intermediate type initialized by the
--   library user to define the different layers of the network.
data LayerDefinition a
LayerDefinition :: (Neuron a) -> Int -> (Connectivity a) -> LayerDefinition a
neuronDef :: LayerDefinition a -> (Neuron a)
neuronCount :: LayerDefinition a -> Int
connect :: LayerDefinition a -> (Connectivity a)

-- | The Layer type, which stores the weight matrix, the bias matrix, and a
--   neuron type.
data Layer a
Layer :: (Matrix a) -> (Vector a) -> (Neuron a) -> Layer a
weightMatrix :: Layer a -> (Matrix a)
biasVector :: Layer a -> (Vector a)
neuron :: Layer a -> (Neuron a)

-- | We have to define a new type to be able to serialize and store
--   networks.
data ShowableLayer a
ShowableLayer :: (Matrix a) -> (Vector a) -> ShowableLayer a
weights :: ShowableLayer a -> (Matrix a)
biases :: ShowableLayer a -> (Vector a)

-- | Connectivity is the type alias for a function that defines the
--   connective matrix for two layers (fully connected, convolutionally
--   connected, etc.)
type Connectivity a = Int -> Int -> Matrix a

-- | A random transformation type alias. It is a transformation defined on
--   an infinite list of uniformly distributed random numbers, and returns
--   a list distributed on the transforming distribution.
type RandomTransform a = [a] -> [a]

-- | We want to be able to convert between layers and showable layers, and
--   vice-versa
layerToShowable :: (Floating (Vector a), Container Vector a, Floating a) => Layer a -> ShowableLayer a

-- | To go from a showable to a layer, we also need a neuron type, which is
--   an unfortunate restriction owed to Haskell's inability to serialize
--   functions.
showableToLayer :: (Floating (Vector a), Container Vector a, Floating a) => (ShowableLayer a, LayerDefinition a) -> Layer a

-- | The createLayer function takes in a random transformation on an
--   infinite stream of uniformly generated numbers, a source of entropy,
--   and two layer definitions, one for the previous layer and one for the
--   next layer. It returns a layer defined by the Layer type -- a weight
--   matrix, a bias vector, and a neuron type.
createLayer :: (RandomGen g, Random a, Floating (Vector a), Container Vector a, Floating a) => RandomTransform a -> g -> LayerDefinition a -> LayerDefinition a -> Layer a
scaleLayer :: (Floating (Vector a), Container Vector a) => a -> Layer a -> Layer a

-- | The connectFully function takes the number of input neurons for a
--   layer, i, and the number of output neurons of a layer, j, and returns
--   an i x j connectivity matrix for a fully connected network.
connectFully :: Int -> Int -> Matrix Float

-- | Initialize an infinite random list given a random transform and a
--   source of entroy.
randomList :: (RandomGen g, Random a, Floating a) => RandomTransform a -> g -> [a]

-- | Define a transformation on the uniform distribution to generate
--   normally distributed numbers in Haskell (the Box-Muller transform)
boxMuller :: Floating a => a -> a -> (a, a)

-- | This is a function of type RandomTransform that transforms a list of
--   uniformly distributed numbers to a list of normally distributed
--   numbers.
normals :: Floating a => [a] -> [a]

-- | A non-transformation to return a list of uniformly distributed numbers
--   from a list of uniformly distributed numbers. It's really a matter of
--   naming consistency. It generates numbers on the range (0, 1]
uniforms :: Floating a => [a] -> [a]

-- | An affine transformation to return a list of uniforms on the range (a,
--   b]
boundedUniforms :: Floating a => (a, a) -> [a] -> [a]
instance (Show a, Element a) => Show (ShowableLayer a)
instance (Element a, Binary a) => Binary (ShowableLayer a)

module Network.Network

-- | Networks are constructed front to back. Start by adding an input
--   layer, then each hidden layer, and finally an output layer.
data Network a
Network :: [Layer a] -> Network a
layers :: Network a -> [Layer a]

-- | The createNetwork function takes in a random transform used for weight
--   initialization, a source of entropy, and a list of layer definitions,
--   and returns a network with the weights initialized per the random
--   transform.
createNetwork :: (RandomGen g, Random a, Floating a, Floating (Vector a), Container Vector a) => RandomTransform a -> g -> [LayerDefinition a] -> Network a

-- | Given a filename, and a list of layer definitions, we want to reexpand
--   the data back into a network.
loadNetwork :: (Binary (ShowableLayer a), Floating a, Floating (Vector a), Container Vector a) => FilePath -> [LayerDefinition a] -> IO (Network a)

-- | Our Unit, an empty network with no layers
emptyNetwork :: Network a

-- | A boolean to check if the network is the unit network or not
isEmptyNetwork :: Network a -> Bool

-- | A function to combine two networks
addNetworks :: (Floating (Vector a), Container Vector a, Product a) => Network a -> Network a -> Network a

-- | Predict folds over each layer of the network using the input vector as
--   the first value of the accumulator. It operates on whatever network
--   you pass in.
predict :: (Floating (Vector a), Container Vector a, Product a) => Vector a -> Network a -> Vector a

-- | A function used in the fold in predict that applies the activation
--   function and pushes the input through a layer of the network.
apply :: (Floating (Vector a), Container Vector a, Product a) => Vector a -> Layer a -> Vector a

-- | Given a filename and a network, we want to save the weights and biases
--   of the network to the file for later use.
saveNetwork :: (Binary (ShowableLayer a), Floating a, Floating (Vector a), Container Vector a) => FilePath -> Network a -> IO ()
instance (Product a, Container Vector a, Floating (Vector a)) => Monoid (Network a)

module Network.Trainer

-- | Trainer is a typeclass for all trainer types - a trainer will take in
--   an instance of itself, a network, a list of training data, and return
--   a new network trained on the data. class Trainer a where fit ::
--   (Floating b) =&gt; a -&gt; Network b -&gt; [TrainingData b] -&gt;
--   Network b
--   
--   A BackpropTrainer performs simple backpropagation on a neural network.
--   It can be used as the basis for more complex trainers.
data BackpropTrainer a
BackpropTrainer :: a -> CostFunction a -> CostFunction' a -> BackpropTrainer a
eta :: BackpropTrainer a -> a
cost :: BackpropTrainer a -> CostFunction a
cost' :: BackpropTrainer a -> CostFunction' a

-- | A CostFunction is used for evaluating a network's performance on a
--   given input
type CostFunction a = Vector a -> Vector a -> a

-- | A CostFunction' (derivative) is used in backPropagation
type CostFunction' a = Vector a -> Vector a -> Vector a

-- | A tuple of (input, expected output)
type TrainingData a = (Vector a, Vector a)

-- | A selection function for performing gradient descent
type Selection a = [TrainingData a] -> [[TrainingData a]]

-- | A predicate (given a network, trainer, a list of training data, and
--   the number of [fit]s performed) that tells the trainer to stop
--   training
type TrainCompletionPredicate a = Network a -> BackpropTrainer a -> [TrainingData a] -> Int -> Bool

-- | Given a network, a trainer, a list of training data, and N, this
--   function trains the network with the list of training data N times
trainNTimes :: (Floating (Vector a), Container Vector a, Product a) => Network a -> BackpropTrainer a -> Selection a -> [TrainingData a] -> Int -> Network a

-- | Given a network, a trainer, a list of training data, and an error
--   value, this function trains the network with the list of training data
--   until the error of the network (calculated by averaging the errors of
--   each training data) is less than the given error value
trainUntilErrorLessThan :: (Floating (Vector a), Container Vector a, Product a, Ord a) => Network a -> BackpropTrainer a -> Selection a -> [TrainingData a] -> a -> Network a

-- | This function trains a network until a given TrainCompletionPredicate
--   is satisfied.
trainUntil :: (Floating (Vector a), Container Vector a, Product a) => Network a -> BackpropTrainer a -> Selection a -> [TrainingData a] -> TrainCompletionPredicate a -> Int -> Network a

-- | The quadratic cost function (1/2) * sum (y - a) ^ 2
quadraticCost :: (Floating (Vector a), Container Vector a) => Vector a -> Vector a -> a

-- | The derivative of the quadratic cost function sum (y - a)
quadraticCost' :: Floating (Vector a) => Vector a -> Vector a -> Vector a

-- | The minibatch function becomes a Selection when partially applied with
--   the minibatch size
minibatch :: (Floating (Vector a), Container Vector a) => Int -> [TrainingData a] -> [[TrainingData a]]

-- | If we want to train the network online
online :: (Floating (Vector a), Container Vector a) => [TrainingData a] -> [[TrainingData a]]

-- | Perform backpropagation on a single training data instance.
backprop :: (Floating (Vector a), Container Vector a, Product a) => BackpropTrainer a -> Network a -> [TrainingData a] -> Network a

-- | The inputs function performs a similar task to outputs, but returns a
--   list of vectors of unactivated inputs
inputs :: (Floating (Vector a), Container Vector a, Product a) => Vector a -> Network a -> [Vector a]

-- | The outputs function scans over each layer of the network and stores
--   the activated results
outputs :: (Floating (Vector a), Container Vector a, Product a) => Vector a -> Network a -> [Vector a]

-- | The deltas function returns a list of layer deltas.
deltas :: (Floating (Vector a), Container Vector a, Product a) => BackpropTrainer a -> Network a -> TrainingData a -> [Vector a]

-- | Compute the hidden layer deltas
hiddenDeltas :: (Floating (Vector a), Container Vector a, Product a) => Network a -> Vector a -> [Vector a] -> [Vector a]

-- | Calculate the nablas for a minibatch and return them as a network (so
--   each weight and bias gets its own nabla).
calculateNablas :: (Floating (Vector a), Container Vector a, Product a) => BackpropTrainer a -> Network a -> Network a -> TrainingData a -> Network a

-- | Declare the BackpropTrainer to be an instance of Trainer. instance
--   (Floating a) =&gt; Trainer (BackpropTrainer a) where
fit :: (Floating (Vector a), Container Vector a, Product a) => Selection a -> BackpropTrainer a -> Network a -> [TrainingData a] -> Network a

-- | Use the cost function to determine the error of a network
evaluate :: (Floating (Vector a), Container Vector a, Product a) => BackpropTrainer a -> Network a -> TrainingData a -> a
