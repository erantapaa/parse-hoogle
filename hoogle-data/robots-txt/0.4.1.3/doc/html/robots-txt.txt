-- Hoogle documentation, generated by Haddock
-- See Hoogle, http://www.haskell.org/hoogle/


-- | Parser for robots.txt
--   
@package robots-txt
@version 0.4.1.3

module Network.HTTP.Robots
type Robot = ([([UserAgent], [Directive])], [Unparsable])
type Unparsable = ByteString
data UserAgent
Wildcard :: UserAgent
Literal :: ByteString -> UserAgent
type Path = ByteString
type TimeInterval = (DiffTime, DiffTime)
data Directive
Allow :: Path -> Directive
Disallow :: Path -> Directive
CrawlDelay :: Rational -> TimeInterval -> Directive
crawlDelay :: Directive -> Rational
timeInterval :: Directive -> TimeInterval
NoArchive :: Path -> Directive
NoSnippet :: Path -> Directive
NoTranslate :: Path -> Directive
NoIndex :: Path -> Directive
subParser :: Parser a -> ByteString -> Parser a
safeParseRational :: Parser Rational
dropUTF8BOM :: ByteString -> ByteString
parseHourMinute :: Parser (Integer, Integer)
parseTimeInterval :: Parser TimeInterval
allDay :: TimeInterval
parseRequestRate :: Parser Directive
parseVisitTime :: Parser Directive
parseCrawlDelay :: Parser Directive
strip :: ByteString -> ByteString

-- | parseRobots is the main entry point for parsing a robots.txt file.
parseRobots :: ByteString -> Either String Robot
robotP :: Parser Robot
unparsableP :: Parser ByteString
agentDirectiveP :: Parser ([UserAgent], [Directive])
skipSpace :: Parser ()
directiveP :: Parser Directive
agentP :: Parser UserAgent
commentsP :: Parser ()
tokenP :: Parser ByteString
tokenWithSpacesP :: Parser ByteString
canAccess :: ByteString -> Robot -> Path -> Bool
instance Show UserAgent
instance Eq UserAgent
instance Show Directive
instance Eq Directive
